{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA_DNN_ImageClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOtLpK457ZKUCAf2ivl6W4Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedicalImageAnalysisTutorials/DeepLearning4All/blob/main/IA_DNN_ImageClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figzmLKBxWC8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook, I will try to provide a practical tutorial for deep learning using simple examples. I will try to use simple implementation and avoid using built-in functions to give clear idea about the concept. You need basic programming knowledge. \n",
        "\n",
        "**The Classification Problem:**\n",
        "\n",
        "We have data represented usually by an array (could be 1D, 2D, or ND). Note the images usually are 3D arrays e.g. with size hight,width,channels.\n",
        "\n",
        "We also have \"labels\" or \"classes\" so for each sample in our data we have its class or label. \n",
        "\n",
        "An example:\n",
        "\n",
        "                 index     sample (X)     label (Y)  \n",
        "                  1         image1          car                      \n",
        "                  2         image2          car                      \n",
        "                  3         image3          dog                               \n",
        "                  4         image4          bike                      \n",
        "                  5         image5          bike                             \n",
        "                  6         image6          dog                      \n",
        "                  .\n",
        "                  .\n",
        "                  . \n",
        "\n",
        "\n",
        "Another example:\n",
        "\n",
        "                 index         sample (X)         label (Y)  \n",
        "                  1         [20 24 25 ... 23]        1                      \n",
        "                  2         [10 20 8  ... 50]        2\n",
        "                  3         [40 24 12 ... 11]        1                      \n",
        "                  4         [12 15 30 ... 60]        1\n",
        "                  5         [19 70 25 ... 22]        1                      \n",
        "                  6         [0  90 23 ... 10]        2\n",
        "                  .\n",
        "                  .\n",
        "                  . \n",
        "\n",
        "\n",
        "We want to find a way to predict the class or the label for new samples that we do not know. \n",
        "\n",
        "There are many machine learning algorithms available e.g. [K-nearest_neighbors  (KNN)](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm or [support vector machine (SVM)](https://en.wikipedia.org/wiki/Support-vector_machine)\n",
        "\n",
        "Here we will investigate the [neural network (NN)](http://neuralnetworksanddeeplearning.com/chap1.html) and the [convolution neural networks (CNN)](https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/) approaches. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooQ5tm85ZDb"
      },
      "source": [
        "# Setup \n",
        "import os, random, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WAn3WH3Lpr"
      },
      "source": [
        "# Image classification using NN\n",
        "\n",
        "I will use two popular public datasets [MINST](http://yann.lecun.com/exdb/mnist/) and [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). Both have 10 classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTv78FC4P_J"
      },
      "source": [
        "## Reading and exploring the datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "Dzj-Tvek4V8s",
        "outputId": "01f00c69-0b32-41bd-ed53-2eaafecf92db"
      },
      "source": [
        "datasetID = 1  # minst is selected by default, for cifar10 use 2\n",
        "NNID      = 2  # NN is by default, for DNN use 2 \n",
        "number_of_classes = 10  # each datasets have 10 classes\n",
        "\n",
        "\n",
        "# minst dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "class_names = range(10)\n",
        "if datasetID==2:\n",
        "    # cifar10 dataset\n",
        "    # The CIFAR10 dataset contains 60,000 color images in 10 classes, \n",
        "    # with 6,000 images in each class.\n",
        "    # The dataset is divided into 50,000 training images and 10,000 testing images.\n",
        "    # The classes are mutually exclusive and there is no overlap between them.\n",
        "\n",
        "    (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# get size \n",
        "h = x_train.shape[1] # image height\n",
        "w = x_train.shape[2] # image width\n",
        "# check for rgb \n",
        "try:\n",
        "    # number of channels\n",
        "    c =  x_train.shape[3]\n",
        "except:\n",
        "    # number of channels\n",
        "    c =  1\n",
        "    # if there is no number of channels, add 1\n",
        "    x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "    y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "    x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "    y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "number_of_pixels = h * w * c\n",
        "\n",
        "# if you have large GPU memory you can combine the images to batches \n",
        "# for faster training.\n",
        "# It is good to try different values\n",
        "batch_size = 64 # you can try larger batch size e.g. 1024 * 6\n",
        "\n",
        "print(\"dataset shape   : \",x_train.shape)\n",
        "print(\"number of images: \",x_train.shape[0])\n",
        "print(\"image size      : \",x_train[0].shape)\n",
        "print(\"image data type : \",type(x_train[0][0][0][0]))\n",
        "print(\"image max  value: \",np.max(x_train[0]))\n",
        "print(\"image min  value: \",np.min(x_train[0]))\n",
        "if c==1:\n",
        "   print(\"gray or binary image (not color image)\")\n",
        "elif c==3:\n",
        "   print(\"rgb color image (or probably non-color image represented with 3 channels)\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# display sample images \n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    #plt.imshow(x_train[i])\n",
        "    plt.imshow(cv2.cvtColor(x_train[i], cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    if datasetID==1:\n",
        "       plt.xlabel(y_train[i])\n",
        "    elif datasetID==2:\n",
        "       plt.xlabel(class_names[y_train[i][0]])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# normalisation\n",
        "x_train = np.array([ x/255.0 for x in x_train])\n",
        "x_val   = np.array([ x/255.0 for x in x_val])\n",
        "#y_train = y_train.astype(np.float32)\n",
        "\n",
        "# for NN we need 1D \n",
        "if NNID ==1:\n",
        "   x_train = np.reshape(x_train, (-1, number_of_pixels))\n",
        "   x_val   = np.reshape(x_val, (-1, number_of_pixels))\n",
        "   x_test  = np.reshape(x_test , (-1, number_of_pixels))\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "#train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset shape   :  (50000, 28, 28, 1)\n",
            "number of images:  50000\n",
            "image size      :  (28, 28, 1)\n",
            "image data type :  <class 'numpy.uint8'>\n",
            "image max  value:  255\n",
            "image min  value:  0\n",
            "gray or binary image (not color image)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  if s != self._text:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 25 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZY4NXWJ4Xoq"
      },
      "source": [
        "## Creating NN model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmSF08Oq4ae6",
        "outputId": "e0424c69-ef1d-4660-c467-fda2be6194f7"
      },
      "source": [
        "# NN\n",
        "def getNNModel(number_of_pixels,number_of_classes):\n",
        "    inputs = keras.Input(shape=(number_of_pixels,), name=\"digits\")\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "print(\"NN model is defined ...\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NN model is defined ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEO2ORyBFeu"
      },
      "source": [
        "## Define optimiser and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2CBVAaABKU4",
        "outputId": "beaf90a5-5671-4429-9a72-bc09d1c19a3d"
      },
      "source": [
        "# Instantiate an optimizer to train the model.\n",
        "\n",
        "optimiserID = 1\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "if optimiserID ==2:\n",
        "   optimizer = keras.optimizers.Adam()#learning_rate=0.0001\n",
        "# Instantiate a loss function.\n",
        "\n",
        "lossFunctionID =1\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric   = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "if lossFunctionID==2:\n",
        "   loss_fn = keras.losses.MeanSquaredError()\n",
        "   # Prepare the metrics.\n",
        "   train_acc_metric = keras.metrics.MeanSquaredError()\n",
        "   val_acc_metric   = keras.metrics.MeanSquaredError()\n",
        "\n",
        "elif  lossFunctionID==3:\n",
        "   loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "   # Prepare the metrics.\n",
        "   train_acc_metric = keras.metrics.CategoricalCrossentropy()\n",
        "   val_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "\n",
        "print(\"optimiser, loss, and metrics are defined .... \")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "optimiser, loss, and metrics are defined .... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxyj5W7P4d2M"
      },
      "source": [
        "## Training our NN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "C8dFra_t4i8d",
        "outputId": "938e39ea-f893-44ed-dfc9-114b94168fa8"
      },
      "source": [
        "# define training parameters and file paths \n",
        "\n",
        "epochs = 2000 # number of iterations \n",
        "\n",
        "# model log files path\n",
        "modelPath   = \"./modelClassification.h5\"\n",
        "logFilePath = \"./training_log.csv\"\n",
        "figPath     = \"./training_log.png\"\n",
        "\n",
        "logFile = open(logFilePath,'w')\n",
        "logFile.write(\"epoch \\t trnLoss \\t valLoss \\t trnAcc \\t valAcc \\t time \\n\" )\n",
        "logFile.close()\n",
        "# Using optimised tensorflow functions provides more speed\n",
        "\n",
        "@tf.function\n",
        "def train_step(model,x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "         logits = model(x, training=True)\n",
        "         #y = keras.utils.to_categorical(y)\n",
        "         loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def val_step(model,x, y):\n",
        "     val_logits = model(x, training=False)\n",
        "     #y = keras.utils.to_categorical(y)\n",
        "     loss_value = loss_fn(y, val_logits)\n",
        "     val_acc_metric.update_state(y, val_logits)\n",
        "     return loss_value\n",
        "\n",
        "\n",
        "model = getNNModel(number_of_pixels,number_of_classes)\n",
        "print(\"===================================================\")\n",
        "print(\"               Training Loop           \")\n",
        "print(\"===================================================\")\n",
        "\n",
        "# we loop number of iterations\n",
        "# for each iteration, we loop through all the training samples\n",
        "for epoch in range(epochs):\n",
        "    #print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(model,x_batch_train, y_batch_train)\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_loss_value = val_step(model,x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    \n",
        "    # compute time required for each epoch\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"epoch:%d \\t trnLoss: %.4f \\t valLoss: %.4f \\t trnAcc: %.4f \\t valAcc: %.4f \\t time:  %.2f\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "    logFile = open(logFilePath,'a')\n",
        "    logFile.write(\"%d \\t %.4f \\t  %.4f \\t %.4f \\t  %.4f \\t  %.2f \\n\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "    logFile.close()\n",
        "    # plot the result\n",
        "    #iaUtils.iaPlotLoss(logFilePath,figPath)\n",
        "\n",
        "# save the final model\n",
        "model.save(modelPath)     "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "               Training Loop           \n",
            "===================================================\n",
            "epoch:0 \t trnLoss: 2.0151 \t valLoss: 2.0413 \t trnAcc: 0.2446 \t valAcc: 0.4060 \t time:  2.30\n",
            "epoch:1 \t trnLoss: 1.6583 \t valLoss: 1.7597 \t trnAcc: 0.5145 \t valAcc: 0.6129 \t time:  1.74\n",
            "epoch:2 \t trnLoss: 1.3197 \t valLoss: 1.3860 \t trnAcc: 0.6474 \t valAcc: 0.7082 \t time:  1.79\n",
            "epoch:3 \t trnLoss: 1.3925 \t valLoss: 1.0749 \t trnAcc: 0.7188 \t valAcc: 0.7693 \t time:  1.74\n",
            "epoch:4 \t trnLoss: 0.7732 \t valLoss: 0.8409 \t trnAcc: 0.7711 \t valAcc: 0.8107 \t time:  1.76\n",
            "epoch:5 \t trnLoss: 0.9789 \t valLoss: 0.6758 \t trnAcc: 0.8036 \t valAcc: 0.8356 \t time:  1.74\n",
            "epoch:6 \t trnLoss: 0.5708 \t valLoss: 0.5551 \t trnAcc: 0.8240 \t valAcc: 0.8502 \t time:  1.78\n",
            "epoch:7 \t trnLoss: 1.0913 \t valLoss: 0.4676 \t trnAcc: 0.8369 \t valAcc: 0.8618 \t time:  1.76\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-1de4d28ea910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    717\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 719\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3119\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m-> 3121\u001b[0;31m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[1;32m   3122\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3123\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKk2JgFo4lgd"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlx1hVMQGAbR"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8X6Da244p-r"
      },
      "source": [
        "# Image classification using CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPkWBAJP47f1"
      },
      "source": [
        "## creating our CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2a1zuKu4_ZR",
        "outputId": "9ef19acc-90d6-4454-c8e5-fd9b91dd80f5"
      },
      "source": [
        "# Simple DNN\n",
        "def getSimpleDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 16 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv2D(nF, (3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x13  = layers.MaxPooling2D((2, 2)) (x11)\n",
        "    x21  = layers.Conv2D(2*nF, (3, 3), activation='relu') (x13)\n",
        "    x23  = layers.MaxPooling2D((2, 2))(x21)\n",
        "    #dense layer for classification\n",
        "    x31 = layers.Flatten()(x23)# convert from 3d to 1d\n",
        "    outputs = layers.Dense(numClass, name=\"predictions\")(x31)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def getDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 64 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv2D(nF, (3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x12  = layers.BatchNormalization()(x11)\n",
        "    x13  = layers.MaxPooling2D((2, 2)) (x12)\n",
        "    x14  = layers.Dropout(0.25)(x13)\n",
        "    x21  = layers.Conv2D(2*nF, (3, 3), activation='relu') (x14)\n",
        "    x22  = layers.BatchNormalization()(x21)\n",
        "    x23  = layers.MaxPooling2D((2, 2))(x22)\n",
        "    x24  = layers.Dropout(0.25)(x23)\n",
        "    x31  = layers.Conv2D(2*nF, (3, 3), activation='relu')(x24)\n",
        "    #dense layer for classification\n",
        "    x41 = layers.Flatten()(x31)# convert from 3d to 1d\n",
        "    #x7 = layers.Dense(2*nF, activation='relu')(x6)\n",
        "    #x8 = layers.Dense(2*nF, activation='relu')(x7)\n",
        "    x42  = layers.Dropout(0.50)(x41)\n",
        "    outputs = layers.Dense(numClass, name=\"predictions\")(x42)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "print(\"DNN model is defined ...\")    \n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DNN model is defined ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_BI-0C4_kr"
      },
      "source": [
        "## Training CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "fKpCUhIT5EWp",
        "outputId": "5409ea04-ca50-4bdd-8dea-373496f7dfaf"
      },
      "source": [
        "# Same code as above \n",
        "\n",
        "epochs = 2000 # number of iterations \n",
        "\n",
        "input_shape = [h,w,c]\n",
        "model = getSimpleDNNModel(input_shape, number_of_pixels,number_of_classes)\n",
        "print(\"===================================================\")\n",
        "print(\"               Training Loop           \")\n",
        "print(\"===================================================\")\n",
        "\n",
        "# we loop number of iterations\n",
        "# for each iteration, we loop through all the training samples\n",
        "for epoch in range(epochs):\n",
        "    #print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(model,x_batch_train, y_batch_train)\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    train_acc_metric.reset_states()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_loss_value = val_step(model,x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    \n",
        "    # compute time required for each epoch\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"epoch:%d \\t trnLoss: %.4f \\t valLoss: %.4f \\t trnAcc: %.4f \\t valAcc: %.4f \\t time:  %.2f\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "    logFile = open(logFilePath,'a')\n",
        "    logFile.write(\"%d \\t %.4f \\t  %.4f \\t %.4f \\t  %.4f \\t  %.2f \\n\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "    logFile.close()\n",
        "    # plot the result\n",
        "    #iaUtils.iaPlotLoss(logFilePath,figPath)\n",
        "\n",
        "# save the final model\n",
        "model.save(modelPath)     "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================================\n",
            "               Training Loop           \n",
            "===================================================\n",
            "epoch:0 \t trnLoss: 2.1686 \t valLoss: 2.1711 \t trnAcc: 0.3517 \t valAcc: 0.3774 \t time:  2.99\n",
            "epoch:1 \t trnLoss: 1.8086 \t valLoss: 1.7737 \t trnAcc: 0.5130 \t valAcc: 0.6611 \t time:  2.34\n",
            "epoch:2 \t trnLoss: 0.7056 \t valLoss: 0.8929 \t trnAcc: 0.7195 \t valAcc: 0.8057 \t time:  2.41\n",
            "epoch:3 \t trnLoss: 0.7747 \t valLoss: 0.5157 \t trnAcc: 0.8156 \t valAcc: 0.8579 \t time:  2.29\n",
            "epoch:4 \t trnLoss: 0.4916 \t valLoss: 0.3548 \t trnAcc: 0.8526 \t valAcc: 0.8774 \t time:  2.42\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-a2a0fac93d01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deIAKC2E5HcT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwUVTMeg5Ks5"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRdRNJF2kwU"
      },
      "source": [
        "# More resources:\n",
        "\n",
        "* [Video tutorial](https://www.youtube.com/watch?v=aircAruvnKk) \n",
        "* [Deep Learning Lecture Winter 20/21 by Prof. Andreas Maier](https://www.youtube.com/watch?v=SCFToE1vM2U&list=PLpOGQvPCDQzvJEPFUQ3mJz72GJ95jyZTh)\n",
        "\n",
        "* https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch\n",
        "* https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d\n",
        "* https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb\n",
        "* https://www.tensorflow.org/tutorials/images/cnn\n"
      ]
    }
  ]
}