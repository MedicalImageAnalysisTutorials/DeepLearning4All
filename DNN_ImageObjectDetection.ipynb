{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN_ImageObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedicalImageAnalysisTutorials/DeepLearning4All/blob/main/DNN_ImageObjectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figzmLKBxWC8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Draft version**\n",
        "In this notebook, I will try to provide a practical tutorial for deep learning using simple examples. I will try to use simple implementation and avoid using built-in functions to give clear idea about the concept. You need basic programming knowledge. \n",
        "\n",
        "**The Object Detection Problem:** involves classification and localization. It takes an image as input and produces one or more bounding boxes with the class label attached to each bounding box. \\\\\n",
        "\n",
        "**The Object Recognition Problem:** is to identity the objects in images or videos. \\\\\n",
        "\n",
        "I made the code flexible so one can try different approaches, datsaets, optimisers, loss functions based on if else statements. \n",
        "\n",
        "The general code structure:\n",
        "\n",
        "    1. Read and pre-process the data\n",
        "    2. Create the model, optimiser, loss function, and metrics\n",
        "    3. Start the training loop\n",
        "    4. Evaluate the final model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooQ5tm85ZDb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "6eb4ba74-84f9-4c74-dacc-d1329bec14fe"
      },
      "source": [
        "# TODO:\n",
        "# complete the introduction\n",
        "# add the introduction with the figures to this notebook https://github.com/idhamari/Deep-Learning-Coursera/blob/master/Convolutional%20Neural%20Networks/Week3/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection_v3a.ipynb\n",
        "# define yolo model and its related functions \n",
        "# find dataset and use it \n",
        "\n",
        "usePytorch = 1\n",
        "import sys\n",
        "# Setup \n",
        "doInstall =1\n",
        "if doInstall:\n",
        "    # !git clone https://github.com/JudasDie/deeplearning.ai/blob/master/Convolutional%20Neural%20Networks/week3/yolo_utils.py\n",
        "    # from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
        "    #!git clone https://github.com/allanzelener/yad2k.git\n",
        "    !git  clone https://github.com/idhamari/YAD2K.git\n",
        "    !pip install SimpleITK\n",
        "    !pip install labelImg \n",
        "#   !git clone https://github.com/tensorflow/models.git \n",
        "#   !wget https://github.com/protocolbuffers/protobuf/releases/download/v3.18.0/protoc-3.18.0-linux-x86_64.zip\n",
        "#   !unzip protoc-3.18.0-linux-x86_64.zip -d protoc-3.18.0\n",
        "  #sys.path.append('/protoc-3.18.0/bin')\n",
        "#   !export PATH=\"protoc-3.18.0/bin:$PATH\"\n",
        "#   !cd models/research\n",
        "#   !protoc object_detection/protos/*.proto --python_out=.\n",
        "#   !cd ../../cocoapi/PythonAPI\n",
        "#   !git clone https://github.com/cocodataset/cocoapi.git\n",
        "#   !make\n",
        "#   !cp -r pycocotools <PATH_TO_TF>/TensorFlow/models/research/\n",
        "    # !cd models/research\n",
        "    # !cp object_detection/packages/tf2/setup.py .\n",
        "    # !python -m pip install --use-feature=2020-resolver .\n",
        "    # !mkdir workspace\n",
        "    # !mkdir workspace/training_demo\n",
        "    # #store all *.csv files and the respective TensorFlow *.record files, which contain the list of annotations for our dataset images.\n",
        "    # !mkdir workspace/training_demo/annotations\n",
        "    # #store exported versions of our trained model(s).\n",
        "    # !mkdir workspace/training_demo/exported-models\n",
        "    # #contain a sub-folder for each of training job. Each subfolder will contain the\n",
        "    # # training pipeline configuration file *.config, as well as all files generated during the training and evaluation of our model.\n",
        "    # !mkdir workspace/training_demo/models\n",
        "    # !mkdir workspace/training_demo/pre-trained-models\n",
        "    # #contains a copy of all the images in our dataset, as well as the respective *.xml files produced for each one, \n",
        "    # # once labelImg is used to annotate objects\n",
        "    # !mkdir workspace/training_demo/images\n",
        "    # !mkdir workspace/training_demo/images/train\n",
        "    # !mkdir workspace/training_demo/images/test\n",
        "import os, random, time, math, colorsys, imghdr, PIL\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import pandas as pd\n",
        "\n",
        "from skimage.transform import resize\n",
        "import cv2 \n",
        "import SimpleITK as sitk \n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Lambda, Conv2D, BatchNormalization\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "\n",
        "# Yolo\n",
        "from yad2k.yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# to reproduce the same results given same input\n",
        "np.random.seed(1)               \n",
        "!ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YAD2K'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (243/243), 2.35 MiB | 11.63 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 4.7 kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.1.1\n",
            "Collecting labelImg\n",
            "  Downloading labelImg-1.8.5.tar.gz (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyqt5\n",
            "  Downloading PyQt5-5.15.4-cp36.cp37.cp38.cp39-abi3-manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from labelImg) (4.2.6)\n",
            "Collecting PyQt5-sip<13,>=12.8\n",
            "  Downloading PyQt5_sip-12.9.0-cp37-cp37m-manylinux1_x86_64.whl (317 kB)\n",
            "\u001b[K     |████████████████████████████████| 317 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting PyQt5-Qt5>=5.15\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.9 MB 35 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: labelImg\n",
            "  Building wheel for labelImg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for labelImg: filename=labelImg-1.8.5-py2.py3-none-any.whl size=258444 sha256=67856c82403a41b7365d85454aa56cfeeb20e176fb37fcbf0bf89eca2a71f48b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/fc/d0/41e6a87f4360fa551173280bd3c871e48711436e7873c6929c\n",
            "Successfully built labelImg\n",
            "Installing collected packages: PyQt5-sip, PyQt5-Qt5, pyqt5, labelImg\n",
            "Successfully installed PyQt5-Qt5-5.15.2 PyQt5-sip-12.9.0 labelImg-1.8.5 pyqt5-5.15.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b59a04fbd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Yolo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myad2k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myad2k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_yolo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myolo_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_boxes_to_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_true_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yad2k'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTipttMz22A0"
      },
      "source": [
        "**The Object Detection Problem:** \\\\\n",
        "When we’re shown an image, our brain instantly recognizes the objects contained in it. On the other hand, it takes a lot of time and training data for a machine to identify these objects. But with the recent advances in hardware and deep learning, this computer vision field has become a whole lot easier and more intuitive.\n",
        "First try to understand what is object detection problem: \\\\\n",
        "\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/I1_2009_09_08_drive_0012_001351-768x223.png)\\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/06/I1_2009_09_08_drive_0012_001351-copy-768x223.png) \\\\\n",
        "Our objective behind doing object detection is two folds:\n",
        "\n",
        "1.   To identify what all objects are present in the image and where they’re located\n",
        "2.   Filter out the object of attention\n",
        "\\\\\n",
        "**Then we can introduce what is object detection using deep learning. We will start from RCNN series net work to Yolo** \\\\\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GQNa5MnGgM6"
      },
      "source": [
        "### RCNN\n",
        "the RCNN algorithm proposes a bunch of boxes in the image and checks if any of these boxes contain any object. **RCNN uses selective search to extract these boxes from an image (these boxes are called regions).**\\\\\n",
        "\n",
        "\n",
        "*   First, an image is taken as an input: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-14-59-02.png)\n",
        "*   Then, we get the Regions of Interest (ROI) using some proposal method (for example, selective search as seen above): \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-00-09.png)\n",
        "*   All these regions are then reshaped as per the input of the CNN, and each region is passed to the ConvNet: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-01-56.png)\n",
        "*   CNN then extracts features for each region and SVMs are used to divide these regions into different classes: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-03-02.png)\n",
        "*   Finally, a bounding box regression (Bbox reg) is used to predict the bounding boxes for each identified region: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-06-33.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI3K0cF0JSLc"
      },
      "source": [
        "##  Fast RCNN \n",
        "Instead of running a CNN 2,000 times per image, we can run it just once per image and get all the regions of interest (regions containing some object).\n",
        "\n",
        "Ross Girshick, the author of RCNN, came up with this idea of running the CNN just once per image and then finding a way to share that computation across the 2,000 regions. In Fast RCNN, we feed the input image to the CNN, which in turn generates the convolutional feature maps. Using these maps, the regions of proposals are extracted. We then use a RoI pooling layer to reshape all the proposed regions into a fixed size, so that it can be fed into a fully connected network. \\\\\n",
        "\n",
        "The differences with RCNN network are as follows: \\\\\n",
        "*   The input image is passed to a ConvNet which returns the region of interests accordingly. Then we apply the RoI pooling layer on the extracted regions of interest to make sure all the regions are of the same size: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-45-26.png)\n",
        "*   Finally, these regions are passed on to a fully connected network which classifies them, as well as returns the bounding boxes using softmax and linear regression layers simultaneously: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-08-15-47-18.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjO3YhWeMHpv"
      },
      "source": [
        "##  Faster RCNN\n",
        "Faster RCNN is the modified version of Fast RCNN. The major difference between them is that Fast RCNN uses selective search for generating Regions of Interest, while Faster RCNN uses “Region Proposal Network”, aka RPN. RPN takes image feature maps as an input and generates a set of object proposals, each with an objectness score as output. The main steps are as follows: \\\\ \n",
        "\n",
        "Finally, the proposals are passed to a fully connected layer which has a softmax layer and a linear regression layer at its top, to classify and output the bounding boxes for objects.\n",
        "\n",
        "1.   We take an image as input and pass it to the ConvNet which returns the feature map for that image.\n",
        "2.   Region proposal network is applied on these feature maps. This returns the object proposals along with their objectness score.\n",
        "3.   A RoI pooling layer is applied on these proposals to bring down all the proposals to the same size.\n",
        "4.   Finally, the proposals are passed to a fully connected layer which has a softmax layer and a linear regression layer at its top, to classify and output the bounding boxes for objects.\n",
        "\n",
        "\n",
        "*   The input image is passed to a ConvNet which returns the region of interests accordingly. Then we apply the RoI pooling layer on the extracted regions of interest to make sure all the regions are of the same size: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/Screenshot-from-2018-10-09-14-15-36.png)\n",
        "*   Let me briefly explain how this Region Proposal Network (RPN) actually works. To begin with, Faster RCNN takes the feature maps from CNN and passes them on to the Region Proposal Network. RPN uses a sliding window over these feature maps, and at each window, it generates k Anchor boxes of different shapes and sizes:: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/10/0n6pZEyvW47nlcdQz.png)\n",
        "\n",
        "Anchor boxes are fixed sized boundary boxes that are placed throughout the image and have different shapes and sizes. For each anchor, RPN predicts two things:\n",
        "\n",
        "*   The first is the probability that an anchor is an object (it does not consider which class the object belongs to)\n",
        "*   Second is the bounding box regressor for adjusting the anchors to better fit the object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31PpH2ziQPvX"
      },
      "source": [
        "##  YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSMEPFtYQcCT"
      },
      "source": [
        "Then we are going to introduce another famous deep learning object detection model: Yolo\n",
        "\n",
        "The YOLO framework (You Only Look Once) deals with object detection in a different way. It takes the entire image in a single instance and predicts the bounding box coordinates and class probabilities for these boxes. The biggest advantage of using YOLO is its superb speed – it’s incredibly fast and can process 45 frames per second. YOLO also understands generalized object representation.\n",
        "\n",
        "This is one of the best algorithms for object detection and has shown a comparatively similar performance to the R-CNN algorithms. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2XfzTP-Fgj5"
      },
      "source": [
        "###  Yolo network structure "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYOcRV7i5QDi"
      },
      "source": [
        "Our model will be trained as follows:\n",
        "\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-18-46-10.png)\n",
        "\n",
        "*   The framework first take the raw image with annotation. Then seperate images as grid(3X3 in our sample): \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-17-46-32.png) \\\\\n",
        "*   Image classification and localization are applied on each grid. YOLO then predicts the bounding boxes and their corresponding class probabilities for objects (if any are found, of course). \n",
        "\n",
        "The labelled data can be encoded as follows: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-18-01-24.png) \\\\\n",
        "Here,\n",
        "*   pc defines whether an object is present in the grid or not (it is the probability)\n",
        "*   bx, by, bh, bw specify the bounding box if there is an object\n",
        "*   c1, c2, c3 represent the classes, obviously, it can have more classes in different datasets. So, if the object is a car, c2 will be 1 and c1 & c3 will be 0, and so on. \\\\\n",
        "\n",
        "Let's take an example, the grid in which we have a car (c2 = 1): \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-19-35-31.png) \\\\\n",
        "\n",
        "The bounding box can be encoded as follows: \\\\\n",
        "![image1](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-18-27-25.png) \\\\\n",
        "\n",
        "pc will be equal to 1. \\\\\n",
        "bx, by, bh, and bw will be calculated relative to this grid only. bx and by are the center point of the object and bh and bw are the ratio of width and height of the bounding box. \\\\\n",
        "In this case, it will be (around) bx = 0.4, by = 0.3, bh = 0.9, bw = 0.5: \\\\\n",
        "\n",
        "![image17](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-15-19-39-51.png) \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqzNLvqwCChS"
      },
      "source": [
        "###  Intersection over Union and Non-Max Suppression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts_ZQKrXCIoh"
      },
      "source": [
        "This is where Intersection over Union comes into the picture. It calculates the intersection over union of the actual bounding box and the predicted bonding box. \\\\\n",
        "\n",
        "![image18](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-16-13-12-02.png) \\\\\n",
        "\n",
        "IoU = Area of the intersection / Area of the union, i.e.\n",
        "\n",
        "IoU = Area of yellow box / Area of green box \\\\\n",
        "\n",
        "One of the most common problems with object detection algorithms is that is that an object might be detected multiple times. To improve the output of YOLO significantly, Non-Max Suppression is used. \\\\\n",
        "\n",
        "![image19](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-16-13-32-40.png) \\\\\n",
        "\n",
        "1. Discard all the boxes having probabilities less than or equal to a pre-defined threshold (for example, 0.5)\n",
        "2. For the remaining boxes:\n",
        "Pick the box with the highest probability and take that as the output \n",
        "prediction. Discard any other box which has IoU greater than the threshold with the output box from the above\n",
        "3. Repeat step 2 until all the boxes are either taken as the output prediction or discarded\n",
        "\n",
        "![image20](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-17-12-21-31.png) \\\\\n",
        "\n",
        "The output are the boxes with maximum probability and suppressing the close-by boxes with non-max probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP4gE04MF9tv"
      },
      "source": [
        "### Anchor Boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn6yEBUQGJEw"
      },
      "source": [
        "Till now, each grid can only identify one object. Considering there may be multiple objects in a single grid, anchor boxes are used. \\\\\n",
        "\n",
        "Each object is assigned to the corresponding grid based on the midpoint of the object and its location. In the example below, we divide the image into 3x3 grid and 2 anchor boxes in each grid.\n",
        "\n",
        "![image21](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-17-13-20-41.png) \\\\\n",
        "\n",
        "So the y label for YOLO with 2 anchor boxes will be: \\\\\n",
        "\n",
        "![image22](https://cdn.analyticsvidhya.com/wp-content/uploads/2018/12/Screenshot-from-2018-11-17-13-33-31.png) \\\\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WAn3WH3Lpr"
      },
      "source": [
        "# Image Object Detection using CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTv78FC4P_J"
      },
      "source": [
        "## Reading and exploring the datasets\n",
        "\n",
        "We are using Face Detection Data Set and Benchmark (FDDB) [dataset](http://vis-www.cs.umass.edu/fddb/README.txt) from university of Massachusetts.  \n",
        "\n",
        "\n",
        "FDDB-folds contains files with names: FDDB-fold-xx.txt and FDDB-fold-xx-ellipseList.txt, where xx = {01, 02, ..., 10} represents the fold-index.\n",
        "\n",
        "Each line in the \"FDDB-fold-xx.txt\" file specifies a path to an image in the above-mentioned data set. For instance, the entry \"2002/07/19/big/img_130\" corresponds to \"originalPics/2002/07/19/big/img_130.jpg.\"\n",
        "\n",
        "The corresponding annotations are included in the file \"FDDB-fold-xx-ellipseList.txt\" in the following \n",
        "format:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<image name i>\n",
        "<number of faces in this image =im>\n",
        "<face i1>\n",
        "<face i2>\n",
        "...\n",
        "<face im>\n",
        "```\n",
        "\n",
        "Here, each face is denoted by:\n",
        "\n",
        "            major_axis_radius minor_axis_radius angle center_x center_y 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzj-Tvek4V8s"
      },
      "source": [
        "datasetID         = 2  # 1:minst is selected by default, for cifar10 use 2\n",
        "NNID              = 4  # 1:NN is by default, for DNN use 2,or 3, for 3D use 4  \n",
        "number_of_classes = 10  # each datasets have 10 classes\n",
        "showSamples = 1\n",
        "doDownload = 0\n",
        "# if you have large GPU memory you can combine the images to batches \n",
        "# for faster training.\n",
        "# It is good to try different values\n",
        "batch_size = 2 # you can try larger batch size e.g. 1024 * 6\n",
        "\n",
        "# Facial dataset\n",
        "# more info http://vis-www.cs.umass.edu/fddb/README.txt\n",
        "\n",
        "if doDownload:\n",
        "    !wget http://vis-www.cs.umass.edu/fddb/originalPics.tar.gz\n",
        "    !tar zxvf  originalPics.tar.gz\n",
        "\n",
        "    !wget http://vis-www.cs.umass.edu/fddb/FDDB-folds.tgz\n",
        "    !tar zxvf FDDB-folds.tgz\n",
        "    !mkdir originalPics\n",
        "    !mv 2002 originalPics\n",
        "    !mv 2003 originalPics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# class_names = range(10)\n",
        "\n",
        "# if datasetID==2:\n",
        "#     # cifar10 dataset\n",
        "#     # The CIFAR10 dataset contains 60,000 color images in 10 classes, \n",
        "#     # with 6,000 images in each class.\n",
        "#     # The dataset is divided into 50,000 training images and 10,000 testing images.\n",
        "#     # The classes are mutually exclusive and there is no overlap between them.\n",
        "\n",
        "#     (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "#     class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#                'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "#     if NNID==4:\n",
        "#         #TODO: fix this \n",
        "#         showSamples =0\n",
        "        \n",
        "#         x_train = x_train.reshape(-1,32*32*3) # (32 * 32 * 3)        \n",
        "#         x_train = np.resize(x_train,(x_train.shape[0],15,15,15))        \n",
        "#         x_train = x_train.reshape(-1,15,15,15)\n",
        "#         x_test = x_test.reshape(-1,32*32*3) # (32 * 32 * 3)\n",
        "#         x_test = np.resize(x_test,(x_test.shape[0],15,15,15))\n",
        "#         x_test = x_test.reshape(-1,15,15,15)\n",
        "#         x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "#         y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "#         x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "#         y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "#         print(x_train.shape)\n",
        "#         print(x_test.shape)\n",
        "\n",
        "\n",
        "# # get size \n",
        "# h = x_train.shape[1] # image height\n",
        "# w = x_train.shape[2] # image width\n",
        "# # check for rgb \n",
        "# try:\n",
        "#     # number of channels\n",
        "#     c =  x_train.shape[3]\n",
        "# except:\n",
        "#     # number of channels\n",
        "#     c =  1\n",
        "#     # if there is no number of channels, add 1\n",
        "#     x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "#     y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "#     x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "#     y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "# # Reserve 10,000 samples for validation.\n",
        "# x_val = x_train[-10000:]\n",
        "# y_val = y_train[-10000:]\n",
        "# x_train = x_train[:-10000]\n",
        "# y_train = y_train[:-10000]\n",
        "\n",
        "# number_of_pixels = h * w * c\n",
        "\n",
        "\n",
        "# print(\"dataset shape   : \",x_train.shape)\n",
        "# print(\"number of images: \",x_train.shape[0])\n",
        "# print(\"image size      : \",x_train[0].shape)\n",
        "# print(\"image data type : \",type(x_train[0][0][0][0]))\n",
        "# print(\"image max  value: \",np.max(x_train[0]))\n",
        "# print(\"image min  value: \",np.min(x_train[0]))\n",
        "# if c==1:\n",
        "#    print(\"gray or binary image (not color image)\")\n",
        "# elif c==3:\n",
        "#    print(\"rgb color image (or probably non-color image represented with 3 channels)\")\n",
        "\n",
        "\n",
        "# # display sample images \n",
        "# if showSamples:\n",
        "#     plt.figure(figsize=(10,10))\n",
        "#     for i in range(25):\n",
        "#         plt.subplot(5,5,i+1)\n",
        "#         plt.xticks([])\n",
        "#         plt.yticks([])\n",
        "#         plt.grid(False)\n",
        "#         #plt.imshow(x_train[i])\n",
        "#         plt.imshow(cv2.cvtColor(x_train[i], cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#         # The CIFAR labels happen to be arrays, \n",
        "#         # which is why you need the extra index\n",
        "#         if datasetID==1:\n",
        "#             plt.xlabel(y_train[i])\n",
        "#         elif datasetID==2:\n",
        "#             plt.xlabel(class_names[y_train[i][0]])\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# # normalisation\n",
        "# x_train = np.array([ x/255.0 for x in x_train])\n",
        "# x_val   = np.array([ x/255.0 for x in x_val])\n",
        "# x_test  = np.array([ x/255.0 for x in x_test])\n",
        "# #y_train = y_train.astype(np.float32)\n",
        "\n",
        "# # for NN we need 1D \n",
        "# if NNID ==1:\n",
        "#    x_train = np.reshape(x_train, (-1, number_of_pixels))\n",
        "#    x_val   = np.reshape(x_val,  (-1, number_of_pixels))\n",
        "#    x_test  = np.reshape(x_test , (-1, number_of_pixels))\n",
        "\n",
        "# # Prepare the training dataset.\n",
        "# print(x_train.shape,y_train.shape)\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "# #train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "\n",
        "# # Prepare the validation dataset.\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "# val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# # Prepare the test dataset.\n",
        "# tst_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "# tst_dataset = tst_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIDkZloTgOkA"
      },
      "source": [
        "import os \n",
        "from glob import glob\n",
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# class_names = range(10)\n",
        "trnRatio = 0.90\n",
        "tstRatio = 1 - trnRatio\n",
        "img_fnms = sorted([y for x in os.walk(\"originalPics\") for y in glob(os.path.join(x[0], '*.jpg'))])\n",
        "lbls_fnms = sorted([y for x in os.walk(\"FDDB-folds\") for y in glob(os.path.join(x[0], '*ellipseList.txt'))])\n",
        "print((img_fnms[0:2]))\n",
        "print(len(img_fnms))\n",
        "print((lbls_fnms[0:2]))\n",
        "print(len(lbls_fnms))\n",
        "print(\"--------------------------\")\n",
        "lblInfo = []\n",
        "numObj = 0\n",
        "for fnm in lbls_fnms:\n",
        "    f = open(fnm,'r')\n",
        "    lines = f.readlines()\n",
        "    imgLoc=[]\n",
        "    for x in lines:\n",
        "        if \"img\" in x: \n",
        "            imgPath = x.strip()\n",
        "        elif \".\" in x:     \n",
        "            imgLoc.append([float(y) for y in x.split()])\n",
        "            if len(imgLoc) == numObj:\n",
        "              lblInfo.append([imgPath,numObj,imgLoc,])                   \n",
        "              imgLoc=[]\n",
        "              numObj=0\n",
        "        else:\n",
        "            numObj = int(x)\n",
        "\n",
        "print(\"---------------------------\")\n",
        "# annotations for 5171 faces\n",
        "# 2845 images  ???\n",
        "print(len(lblInfo))\n",
        "s = 0 \n",
        "for x in lblInfo:\n",
        "#     print(x[0])\n",
        "#     print(x[1])\n",
        "      s = s  + x[1]\n",
        "#     for i in range (x[1]):\n",
        "#         print(x[2][i])\n",
        "print(s)\n",
        "# imgLst = [1             , 2               ]\n",
        "# lblLst = [[1,3,[loc]]   , [1,3,[loc]]     ]\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test)\n",
        "# class_names = range(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP0E_Pzs04y_"
      },
      "source": [
        "## Dataset augmentation\n",
        "\n",
        "It is important to train the model on different variations of the dataset. It is also important to have large datset for training.\n",
        "\n",
        "Using dataset augmentation helps to achieve both of the above goals. From one image, one can generate hundred thousands of images using image transformation.\n",
        "\n",
        "The image transformation could be [spatial transform]() or point transform where we move the points of the image to new locations e.g. shifting, flipping, and/or rotating the imag. \n",
        "\n",
        "Another type of transformation is intensity transform or pixel transform where we change the color values of the pixels in the image e.g. invert the color, add more brightness or darkness. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb2KdoTR07L2"
      },
      "source": [
        "# TODO\n",
        "doAug = 1\n",
        "\n",
        "def imagePixelTransforms(img):    \n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    img1   = 1.0- img # invert color\n",
        "    img2   = img +0.3 # more brightness\n",
        "    img3   = img -0.3 # more darkness\n",
        "    images = np.array([img1,img2,img3])\n",
        "    images = [ img.reshape(img.shape) for img in images]\n",
        "\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    return images\n",
        "\n",
        "def imagePointTransforms(img):\n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    # Perform the rotation\n",
        "    center  = (img.shape[0] / 2, img.shape[1] / 2)\n",
        "    sz      = (img.shape[1], img.shape[0])\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 45, 1)\n",
        "    img1 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img1 = img1[...,np.newaxis] if img1.shape !=img.shape else img1\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 90, 1)\n",
        "    img2 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img2 = img2[...,np.newaxis] if img2.shape !=img.shape else img2\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 270, 1)\n",
        "    img3 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img3 = img3[...,np.newaxis] if img3.shape !=img.shape else img3\n",
        "\n",
        "    images = np.array([img1,img2,img3])\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    #plt.figure() ;    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # plt.figure() ;    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    return images\n",
        "\n",
        "\n",
        "# define a function for sitk transform\n",
        "def resample(img_array, transform):\n",
        "    # Output image Origin, Spacing, Size, Direction are taken from the reference\n",
        "    # image in this call to Resample\n",
        "    image = sitk.GetImageFromArray(img_array)\n",
        "    reference_image = image\n",
        "    interpolator = sitk.sitkCosineWindowedSinc\n",
        "    default_value = 100.0\n",
        "    resampled_img = sitk.Resample(image, reference_image, transform,\n",
        "                         interpolator, default_value)\n",
        "    resampled_array = sitk.GetArrayFromImage(resampled_img)\n",
        "    return resampled_array\n",
        "\n",
        "def affine_rotate(transform, degrees):\n",
        "    parameters = np.array(transform.GetParameters())\n",
        "    new_transform = sitk.AffineTransform(transform)\n",
        "    dimension =3 \n",
        "    matrix = np.array(transform.GetMatrix()).reshape((dimension,dimension))\n",
        "    radians = -np.pi * degrees / 180.\n",
        "    rotation = np.array([[1  ,0,0], \n",
        "                         [0, np.cos(radians), -np.sin(radians)],\n",
        "                         [0, np.sin(radians), np.cos(radians)]]\n",
        "                        )\n",
        "    new_matrix = np.dot(rotation, matrix)\n",
        "    new_transform.SetMatrix(new_matrix.ravel())\n",
        "    return new_transform\n",
        "\n",
        "\n",
        "def imagePoint3DTransforms(img):\n",
        "    #print(\"imagePoint3DTransforms\")\n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    # Perform the rotation\n",
        "    # In SimpleITK resampling convention, the transformation maps points \n",
        "    # from the fixed image to the moving image,\n",
        "    # so inverse of the transform is applied\n",
        "\n",
        "    center = (img.shape[0] /2, img.shape[1] /2,img.shape[1] /2)\n",
        "    rotation_around_center = sitk.AffineTransform(3)\n",
        "    rotation_around_center.SetCenter(center)\n",
        "    \n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -45)\n",
        "    img1 = resample(img, rotation_around_center)\n",
        "\n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -90)\n",
        "    img2 = resample(img, rotation_around_center)\n",
        "\n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -90)\n",
        "    img3 = resample(img, rotation_around_center)\n",
        "\n",
        "    images = np.array([img1,img2,img3])\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    #plt.figure() ;    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # plt.figure() ;    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    return images\n",
        "\n",
        "def doAugmentation(images,labels,batch_size):\n",
        "    # input is an image or a batch e.g. list of images \n",
        "    # get numpy arrays from the tensor    \n",
        "    images = images.numpy()\n",
        "    labels = labels.numpy()\n",
        "    # if 1d convert back to 2d\n",
        "    #print(images.shape)\n",
        "    rgb = 0 ; is3d = 0\n",
        "    if len(images.shape) == 2:\n",
        "       try: \n",
        "          img2d_shape = int(math.sqrt(images.shape[1])) # gray or binary image\n",
        "          images =images.reshape(-1,img2d_shape,img2d_shape)\n",
        "       except:\n",
        "          try: \n",
        "            img2d_shape = int(math.sqrt(images.shape[1]/3)) # rgb image\n",
        "            images =images.reshape(-1,img2d_shape,img2d_shape,3)\n",
        "            rgb = 1  \n",
        "          except:\n",
        "            pass  \n",
        "            # img3d_shape = int(math.sqrt(images.shape[1]/3)) # rgb image\n",
        "            # images =images.reshape(-1,img2d_shape,img2d_shape,3)\n",
        "            # is3d = 1  \n",
        "\n",
        "\n",
        "\n",
        "    x_outputs = [] ; y_outputs = []\n",
        "    i = 0\n",
        "    for img in images:\n",
        "        #print(\"-------------------------\", i ,\"--------------------\")\n",
        "        if NNID==4:\n",
        "           img = img.squeeze() \n",
        "        # from each images we generate 6 images\n",
        "        # 64 batch will generate 448\n",
        "        x_outputs.extend([img])\n",
        "        imgs1 = imagePoint3DTransforms(img)\n",
        "        imgs2 = imagePixelTransforms(img)\n",
        "        #if not rgb:\n",
        "           #imgs1 = np.array( x[...,np.newaxis] for x in imgs1 if len(x.shape)<3) \n",
        "           #imgs2 = np.array( x[...,np.newaxis] for x in imgs2 if len(x.shape)<3)\n",
        "        x_outputs.extend(imgs1) # 3 images\n",
        "        x_outputs.extend(imgs2) # 3 images\n",
        "        # print(img.shape)\n",
        "        # print(imgs1[0].shape)\n",
        "        # print(imgs2[0].shape)\n",
        "        # assign the same label to all transformed images\n",
        "        for j in range ( len(imgs1) +len(imgs2)+1):\n",
        "            y_outputs.extend([labels[i]])\n",
        "\n",
        "        i = i +1\n",
        "    x_outputs = np.array(x_outputs)\n",
        "    if NNID==4:\n",
        "       x_outputs = np.array([x[...,np.newaxis] for x in x_outputs])\n",
        "    y_outputs = np.array(y_outputs)\n",
        "\n",
        "    if (not rgb) and (NNID==1):\n",
        "       x_outputs = np.reshape(x_outputs, (-1,img2d_shape*img2d_shape,1))\n",
        "    elif (rgb) and (NNID==1):\n",
        "       x_outputs = np.reshape(x_outputs, (-1,img2d_shape*img2d_shape*3))   \n",
        "\n",
        "    new_train_dataset = tf.data.Dataset.from_tensor_slices((x_outputs, y_outputs))\n",
        "    new_train_dataset = new_train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    return new_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmSF08Oq4ae6"
      },
      "source": [
        "# NN TensorFlow\n",
        "def getNNModel(number_of_pixels,number_of_classes):\n",
        "    inputs = keras.Input(shape=(number_of_pixels,), name=\"digits\")\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "print(\"NN model is defined ...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpxH1ntsqfuZ"
      },
      "source": [
        "# TODO\n",
        "# NN pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEO2ORyBFeu"
      },
      "source": [
        "## Define optimiser and loss function for NN and CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQbxXtlvGj1r"
      },
      "source": [
        "# defined functions of Yolo\n",
        "\n",
        "def xywh2minmax(xy, wh):\n",
        "    xy_min = xy - wh / 2\n",
        "    xy_max = xy + wh / 2\n",
        "\n",
        "    return xy_min, xy_max\n",
        "\n",
        "\n",
        "def iou(pred_mins, pred_maxes, true_mins, true_maxes):\n",
        "    intersect_mins = K.maximum(pred_mins, true_mins)\n",
        "    intersect_maxes = K.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "\n",
        "    pred_wh = pred_maxes - pred_mins\n",
        "    true_wh = true_maxes - true_mins\n",
        "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
        "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores = intersect_areas / union_areas\n",
        "\n",
        "    return iou_scores\n",
        "\n",
        "\n",
        "def yolo_head(feats):\n",
        "    # Dynamic implementation of conv dims for fully convolutional model.\n",
        "    conv_dims = K.shape(feats)[1:3]  # assuming channels last\n",
        "    # In YOLO the height index is the inner most iteration.\n",
        "    conv_height_index = K.arange(0, stop=conv_dims[0])\n",
        "    conv_width_index = K.arange(0, stop=conv_dims[1])\n",
        "    conv_height_index = K.tile(conv_height_index, [conv_dims[1]])\n",
        "\n",
        "    # TODO: Repeat_elements and tf.split doesn't support dynamic splits.\n",
        "    # conv_width_index = K.repeat_elements(conv_width_index, conv_dims[1], axis=0)\n",
        "    conv_width_index = K.tile(\n",
        "        K.expand_dims(conv_width_index, 0), [conv_dims[0], 1])\n",
        "    conv_width_index = K.flatten(K.transpose(conv_width_index))\n",
        "    conv_index = K.transpose(K.stack([conv_height_index, conv_width_index]))\n",
        "    conv_index = K.reshape(conv_index, [1, conv_dims[0], conv_dims[1], 1, 2])\n",
        "    conv_index = K.cast(conv_index, K.dtype(feats))\n",
        "\n",
        "    conv_dims = K.cast(K.reshape(conv_dims, [1, 1, 1, 1, 2]), K.dtype(feats))\n",
        "\n",
        "    box_xy = (feats[..., :2] + conv_index) / conv_dims * 448\n",
        "    box_wh = feats[..., 2:4] * 448\n",
        "\n",
        "    return box_xy, box_wh\n",
        "\n",
        "def yolo_loss(y_true, y_pred):\n",
        "    label_class   = y_true[..., :20]  # ? * 7 * 7 * 20\n",
        "    label_box     = y_true[..., 20:24]  # ? * 7 * 7 * 4\n",
        "    response_mask = y_true[..., 24]  # ? * 7 * 7\n",
        "    response_mask = K.expand_dims(response_mask)  # ? * 7 * 7 * 1\n",
        "\n",
        "    predict_class  = y_pred[..., :20]  # ? * 7 * 7 * 20\n",
        "    predict_trust  = y_pred[..., 20:22]  # ? * 7 * 7 * 2\n",
        "    predict_box    = y_pred[..., 22:]  # ? * 7 * 7 * 8\n",
        "\n",
        "    _label_box    = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
        "    _predict_box  = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
        "\n",
        "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
        "    label_xy = K.expand_dims(label_xy, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
        "    label_wh = K.expand_dims(label_wh, 3)  # ? * 7 * 7 * 1 * 1 * 2\n",
        "    label_xy_min, label_xy_max = xywh2minmax(label_xy, label_wh)  # ? * 7 * 7 * 1 * 1 * 2, ? * 7 * 7 * 1 * 1 * 2\n",
        "\n",
        "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
        "    predict_xy = K.expand_dims(predict_xy, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
        "    predict_wh = K.expand_dims(predict_wh, 4)  # ? * 7 * 7 * 2 * 1 * 2\n",
        "    predict_xy_min, predict_xy_max = xywh2minmax(predict_xy, predict_wh)  # ? * 7 * 7 * 2 * 1 * 2, ? * 7 * 7 * 2 * 1 * 2\n",
        "\n",
        "    iou_scores = iou(predict_xy_min, predict_xy_max, label_xy_min, label_xy_max)  # ? * 7 * 7 * 2 * 1\n",
        "    best_ious = K.max(iou_scores, axis=4)  # ? * 7 * 7 * 2\n",
        "    best_box = K.max(best_ious, axis=3, keepdims=True)  # ? * 7 * 7 * 1\n",
        "\n",
        "    box_mask = K.cast(best_ious >= best_box, K.dtype(best_ious))  # ? * 7 * 7 * 2\n",
        "\n",
        "    no_object_loss = 0.5 * (1 - box_mask * response_mask) * K.square(0 - predict_trust)\n",
        "    object_loss = box_mask * response_mask * K.square(1 - predict_trust)\n",
        "    confidence_loss = no_object_loss + object_loss\n",
        "    confidence_loss = K.sum(confidence_loss)\n",
        "\n",
        "    class_loss = response_mask * K.square(label_class - predict_class)\n",
        "    class_loss = K.sum(class_loss)\n",
        "\n",
        "    _label_box = K.reshape(label_box, [-1, 7, 7, 1, 4])\n",
        "    _predict_box = K.reshape(predict_box, [-1, 7, 7, 2, 4])\n",
        "\n",
        "    label_xy, label_wh = yolo_head(_label_box)  # ? * 7 * 7 * 1 * 2, ? * 7 * 7 * 1 * 2\n",
        "    predict_xy, predict_wh = yolo_head(_predict_box)  # ? * 7 * 7 * 2 * 2, ? * 7 * 7 * 2 * 2\n",
        "\n",
        "    box_mask = K.expand_dims(box_mask)\n",
        "    response_mask = K.expand_dims(response_mask)\n",
        "\n",
        "    box_loss  = 5 * box_mask * response_mask * K.square((label_xy - predict_xy) / 448)\n",
        "    box_loss += 5 * box_mask * response_mask * K.square((K.sqrt(label_wh) - K.sqrt(predict_wh)) / 448)\n",
        "    box_loss = K.sum(box_loss)\n",
        "\n",
        "    loss = confidence_loss + class_loss + box_loss\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2CBVAaABKU4"
      },
      "source": [
        "# Instantiate an optimizer to train the model.\n",
        "\n",
        "optimiserID = 1 # SGD by default for ADAM use 2 \n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "if optimiserID ==2:\n",
        "   optimizer = keras.optimizers.Adam()#learning_rate=0.0001\n",
        "# Instantiate a loss function.\n",
        "\n",
        "lossFunctionID = 1 # SparseCategoricalCrossentropy by default for MSE use 2 \n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric   = keras.metrics.SparseCategoricalAccuracy()\n",
        "tst_acc_metric   = keras.metrics.SparseCategoricalAccuracy()\n",
        "if lossFunctionID==2:\n",
        "   loss_fn = keras.losses.MeanSquaredError()\n",
        "   # Prepare the metrics.\n",
        "   train_acc_metric = keras.metrics.MeanSquaredError()\n",
        "   val_acc_metric   = keras.metrics.MeanSquaredError()\n",
        "   tst_acc_metric   = keras.metrics.MeanSquaredError()\n",
        "\n",
        "elif lossFunctionID==3:\n",
        "   loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "   # Prepare the metrics.\n",
        "   train_acc_metric = keras.metrics.CategoricalCrossentropy()\n",
        "   val_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "   tst_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "elif lossFunctionID==4:\n",
        "   loss_fn = yolo_loss\n",
        "  #  # Prepare the metrics.\n",
        "  #  train_acc_metric = keras.metrics.CategoricalCrossentropy()\n",
        "  #  val_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "  #  tst_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "print(\"optimiser, loss, and metrics are defined .... \")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmAbPxuDqrVV"
      },
      "source": [
        "# TODO\n",
        "# Pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxyj5W7P4d2M"
      },
      "source": [
        "\n",
        "## Define required functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8dFra_t4i8d"
      },
      "source": [
        "\n",
        "# define training parameters and file paths \n",
        "\n",
        "# model log files path\n",
        "modelPath   = \"./modelClassification.h5\"\n",
        "logFilePath = \"./training_log.csv\"\n",
        "figPath     = \"./training_log.png\"\n",
        "\n",
        "logFile = open(logFilePath,'w')\n",
        "logFile.write(\"epoch \\t trnLoss \\t valLoss \\t trnAcc \\t valAcc \\t time \\n\" )\n",
        "logFile.close()\n",
        "# Using optimised tensorflow functions provides more speed\n",
        "\n",
        "@tf.function\n",
        "def train_step(model,x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        #print(\"get result\")\n",
        "        model_output = model(x, training=True)\n",
        "        #print(\"get loss value \")\n",
        "        #y = keras.utils.to_categorical(y)\n",
        "        loss_value = loss_fn(y, model_output)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def val_step(model,x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    #y = keras.utils.to_categorical(y)\n",
        "    loss_value = loss_fn(y, val_logits)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "    return loss_value\n",
        "\n",
        "# plotting function to monitor the curves\n",
        "def iaPlotLoss(logPath,figPath=None):\n",
        "    f = open(logPath,'r')\n",
        "    lst = f.readlines()\n",
        "    # first line is labels:\n",
        "    labels = lst[0].split()[1:-2]\n",
        "    x  = [ int(  ln.split()[0]) for ln in lst[1:]] # epoch\n",
        "    y1 = [ float(ln.split()[1]) for ln in lst[1:]] # lossTrain\n",
        "    y2 = [ float(ln.split()[2]) for ln in lst[1:]] # lossValidation\n",
        "    y3 = [ float(ln.split()[3]) for ln in lst[1:]] # accTrain\n",
        "    y4 = [ float(ln.split()[4]) for ln in lst[1:]] # accValidation\n",
        "    #plotting    \n",
        "    plt.clf()\n",
        "    fig, ax = plt.subplots()    \n",
        "    l1, = ax.plot(x, y1) ;     l2, = ax.plot(x, y2) ;\n",
        "    l3, = ax.plot(x, y3) ;     l4, = ax.plot(x, y4) ;\n",
        "    ax.legend((l1, l2,l3,l4), labels, loc='upper right', shadow=True)\n",
        "    plt.xlabel('epoch')\n",
        "    if figPath:\n",
        "        plt.savefig(figPath, bbox_inches='tight')\n",
        "    else:\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "#================================================\n",
        "#                  YOLO\n",
        "#================================================\n",
        "def read_classes(classes_path):\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def read_anchors(anchors_path):\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "        anchors = [float(x) for x in anchors.split(',')]\n",
        "        anchors = np.array(anchors).reshape(-1, 2)\n",
        "    return anchors\n",
        "\n",
        "def generate_colors(class_names):\n",
        "    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "    random.seed(10101)      # Fixed seed for consistent colors across runs.\n",
        "    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "    random.seed(None)       # Reset seed to default.\n",
        "    return colors\n",
        "\n",
        "def scale_boxes(boxes, image_shape):\n",
        "    \"\"\" Scales the predicted boxes in order to be drawable on the image\"\"\"\n",
        "    height = image_shape[0]\n",
        "    width = image_shape[1]\n",
        "    image_dims = K.stack([height, width, height, width])\n",
        "    image_dims = K.reshape(image_dims, [1, 4])\n",
        "    boxes = boxes * image_dims\n",
        "    return boxes\n",
        "\n",
        "def preprocess_image(img_path, model_image_size):\n",
        "    image_type = imghdr.what(img_path)\n",
        "    image = Image.open(img_path)\n",
        "    resized_image = image.resize(tuple(reversed(model_image_size)), Image.BICUBIC)\n",
        "    image_data = np.array(resized_image, dtype='float32')\n",
        "    image_data /= 255.\n",
        "    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "    return image, image_data\n",
        "\n",
        "def draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors):   \n",
        "    font = ImageFont.truetype(font='font/FiraMono-Medium.otf',size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "    thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "    for i, c in reversed(list(enumerate(out_classes))):\n",
        "        predicted_class = class_names[c]\n",
        "        box = out_boxes[i]\n",
        "        score = out_scores[i]\n",
        "\n",
        "        label = '{} {:.2f}'.format(predicted_class, score)\n",
        "\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        label_size = draw.textsize(label, font)\n",
        "\n",
        "        top, left, bottom, right = box\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "        print(label, (left, top), (right, bottom))\n",
        "\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
        "        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        del draw\n",
        "\n",
        "\n",
        "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
        "    box_scores = box_confidence*box_class_probs\n",
        "    box_classes = K.argmax(box_scores,-1)\n",
        "    box_class_scores = K.max(box_scores,-1)\n",
        "    filtering_mask = box_class_scores>threshold\n",
        "    scores = tf.boolean_mask(box_class_scores,filtering_mask)\n",
        "    boxes = tf.boolean_mask(boxes,filtering_mask)\n",
        "    classes = tf.boolean_mask(box_classes,filtering_mask)\n",
        " \n",
        "    return scores, boxes, classes      \n",
        "\n",
        "# Intersection over Union\n",
        "# check if predicted box is good iou>0.5\n",
        "def iou(box1, box2):\n",
        "    xi1 = max(box1[0],box2[0])\n",
        "    yi1 = max(box1[1],box2[1])\n",
        "    xi2 = min(box1[2],box2[2])\n",
        "    yi2 = min(box1[3],box2[3])\n",
        "    inter_area = (yi2-yi1)*(xi2-xi1)\n",
        "    box1_area = (box1[3]-box1[1])*(box1[2]-box1[0])\n",
        "    box2_area = (box2[3]-box2[1])*(box2[2]-box2[0])\n",
        "    union_area = box1_area+box2_area-inter_area\n",
        "    iou = inter_area/union_area\n",
        "    return iou  \n",
        "\n",
        "# find the best box from number of detcted boxes\n",
        "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
        "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n",
        "    K.get_session().run(tf.variables_initializer([max_boxes_tensor]))\n",
        "    nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes,iou_threshold)\n",
        "    scores = K.gather(scores,nms_indices)\n",
        "    boxes = K.gather(boxes,nms_indices)\n",
        "    classes = K.gather(classes,nms_indices)\n",
        "    return scores, boxes, classes    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mox2km11qyqY"
      },
      "source": [
        "##TODO pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlx1hVMQGAbR"
      },
      "source": [
        "if NNID==1:\n",
        "    # Load the saved model \n",
        "    model = keras.models.load_model(modelPath, compile=False)\n",
        "\n",
        "    start_time = time.time() \n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_tst, y_batch_tst in tst_dataset:\n",
        "        output = model.predict(x_batch_tst)\n",
        "        #y = keras.utils.to_categorical(y_batch_tst)\n",
        "        tst_acc_metric.update_state(y_batch_tst, output)\n",
        "\n",
        "    tst_acc = tst_acc_metric.result()\n",
        "\n",
        "    # compute time required for each epoch\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"test accuracy : %.4f \\t time:  %.2f\" % (  float(tst_acc), end_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZmXOUfrDho"
      },
      "source": [
        "# TODO Pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPkWBAJP47f1"
      },
      "source": [
        "## Creating CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2a1zuKu4_ZR"
      },
      "source": [
        "# Simple DNN\n",
        "# just two conolution layers followed by dense layer\n",
        "def getSimpleDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 16 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv2D(nF, (3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x13  = layers.MaxPooling2D((2, 2)) (x11)\n",
        "    x21  = layers.Conv2D(2*nF, (3, 3), activation='relu') (x13)\n",
        "    x23  = layers.MaxPooling2D((2, 2))(x21)\n",
        "    #dense layer for classification\n",
        "    x31 = layers.Flatten()(x23)# convert from 3d to 1d\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x31)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# this is a better model for CIFAR10\n",
        "def getDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 64 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv2D(nF, (3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x12  = layers.BatchNormalization()(x11)\n",
        "    x13  = layers.MaxPooling2D((2, 2)) (x12)\n",
        "    x14  = layers.Dropout(0.25)(x13)\n",
        "    x21  = layers.Conv2D(2*nF, (3, 3), activation='relu') (x14)\n",
        "    x22  = layers.BatchNormalization()(x21)\n",
        "    x23  = layers.MaxPooling2D((2, 2))(x22)\n",
        "    x24  = layers.Dropout(0.25)(x23)\n",
        "    x31  = layers.Conv2D(2*nF, (3, 3), activation='relu')(x24)\n",
        "    #dense layer for classification\n",
        "    x41 = layers.Flatten()(x31)# convert from 3d to 1d\n",
        "    #x7 = layers.Dense(2*nF, activation='relu')(x6)\n",
        "    #x8 = layers.Dense(2*nF, activation='relu')(x7)\n",
        "    x42  = layers.Dropout(0.50)(x41)\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x42)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def getSimpleDNN3DModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 16 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv3D(nF, (3, 3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x13  = layers.MaxPooling3D((2, 2, 2)) (x11)\n",
        "    x21  = layers.Conv3D(2*nF, (3, 3, 3), activation='relu') (x13)\n",
        "    x23  = layers.MaxPooling3D((2, 2 ,2))(x21)\n",
        "    #dense layer for classification\n",
        "    x31 = layers.Flatten()(x23)# convert from 3d to 1d\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x31)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "#========================================\n",
        "#              Yolo  \n",
        "#========================================\n",
        "\n",
        "#\n",
        "# tf.keras.layers.Conv2D(\n",
        "#     filters, kernel_size, strides=(1, 1), padding='valid',\n",
        "#     data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n",
        "#     use_bias=True, kernel_initializer='glorot_uniform',\n",
        "#     bias_initializer='zeros', kernel_regularizer=None,\n",
        "#     bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n",
        "#     bias_constraint=None, **kwargs\n",
        "# )\n",
        "#\n",
        "\n",
        "# architecture_config = [\n",
        "#     (7, 64, 2, 3),\n",
        "#     \"M\",\n",
        "#     (3, 192, 1, 1),\n",
        "#     \"M\",\n",
        "#     (1, 128, 1, 0),\n",
        "#     (3, 256, 1, 1),\n",
        "#     (1, 256, 1, 0),\n",
        "#     (3, 512, 1, 1),\n",
        "#     \"M\",\n",
        "#     [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
        "#     (1, 512, 1, 0),\n",
        "#     (3, 1024, 1, 1),\n",
        "#     \"M\",\n",
        "#     [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
        "#     (3, 1024, 1, 1),\n",
        "#     (3, 1024, 2, 1),\n",
        "#     (3, 1024, 1, 1),\n",
        "#     (3, 1024, 1, 1),\n",
        "# ]\n",
        "\n",
        "def getYoloDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    # input_shape = [batch_size, width, height, num of channels]\n",
        "    nF        = 16 # number of filters\n",
        "    lrelu = layers.LeakyReLU(alpha=0.1)\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\")\n",
        "    # Create Yolo model\n",
        "    x011  = layers.Conv2D(nF*4, (7, 7), 2, padding = 'same', activation=lrelu, input_shape=input_shape) (inputs)\n",
        "    x012  = layers.MaxPooling2D((2, 2), 2, padding = 'same') (x011)\n",
        "   \n",
        "    x021  = layers.Conv2D(nF*12, (3, 3), 1, padding = 'same', activation=lrelu) (x012)\n",
        "    x022  = layers.MaxPooling2D((2, 2), 2, padding = 'same') (x021)\n",
        "    \n",
        "    x030  = layers.Conv2D(nF*8, (1, 1), 1, padding = 'same', activation=lrelu)  (x022)\n",
        "    x040  = layers.Conv2D(nF*16, (3, 3), 1, padding = 'same', activation=lrelu) (x030)\n",
        "    x050  = layers.Conv2D(nF*16, (1, 1), 1, padding = 'same', activation=lrelu) (x040)\n",
        "    \n",
        "    x061  = layers.Conv2D(nF*32, (3, 3), 1, padding = 'same', activation=lrelu) (x050)\n",
        "    x062  = layers.MaxPooling2D((2, 2), padding = 'same', 2) (x061)\n",
        "    \n",
        "    x070  = layers.Conv2D(nF*16, (1, 1), 1, padding = 'same', activation=lrelu) (x062)\n",
        "    x080  = layers.Conv2D(nF*32, (3, 3), 1, padding = 'same', activation=lrelu) (x070)\n",
        "    x090  = layers.Conv2D(nF*16, (1, 1), 1, padding = 'same', activation=lrelu) (x080)\n",
        "    x010  = layers.Conv2D(nF*32, (3, 3), 1, padding = 'same', activation=lrelu) (x090)\n",
        "    x110  = layers.Conv2D(nF*16, (1, 1), 1, padding = 'same', activation=lrelu) (x010)\n",
        "    x120  = layers.Conv2D(nF*32, (3, 3), 1, padding = 'same', activation=lrelu) (x110)\n",
        "    x130  = layers.Conv2D(nF*16, (1, 1), 1, padding = 'same', activation=lrelu) (x120)\n",
        "    x140  = layers.Conv2D(nF*32, (3, 3), 1, padding = 'same', activation=lrelu) (x130)\n",
        "    x150  = layers.Conv2D(nF*32, (1, 1), 1, padding = 'same', activation=lrelu) (x140)\n",
        "\n",
        "    x161  = layers.Conv2D(nF*64, (3, 3), 1, padding = 'same', activation=lrelu) (x150)\n",
        "    x162  = layers.MaxPooling2D((2, 2), 2, padding = 'same') (x161)\n",
        "\n",
        "    x170  = layers.Conv2D(nF*32, (1, 1), 1, padding = 'same', activation=lrelu) (x162)\n",
        "    x180  = layers.Conv2D(nF*64, (3, 3), 1, padding = 'same', activation=lrelu) (x170)\n",
        "    x190  = layers.Conv2D(nF*32, (1, 1), 1, padding = 'same', activation=lrelu) (x180)\n",
        "    x200  = layers.Conv2D(nF*64, (3, 3), 1, padding = 'same', activation=lrelu) (x190)\n",
        "    x210  = layers.Conv2D(nF*64, (3, 3), 1, padding = 'same', activation=lrelu) (x200)\n",
        "    x220  = layers.Conv2D(nF*64, (3, 3), 2, padding = 'same', activation=lrelu) (x210)\n",
        "    x230  = layers.Conv2D(nF*64, (3, 3), 1, padding = 'same', activation=lrelu) (x220)\n",
        "    x240  = layers.Conv2D(nF*64, (3, 3), 1, padding = 'same', activation=lrelu) (x230)\n",
        "\n",
        "    x250   = layers.Flatten()(x23)# convert from 3d to 1d\n",
        "    x260   = layers.Dense(number_of_classes, name=\"predictions1\")(x250)\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions2\")(x260)\n",
        "\n",
        "\n",
        "    #dense layer for classification\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model    \n",
        "\n",
        "\n",
        "print(\"DNN model is defined ...\")    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_BI-0C4_kr"
      },
      "source": [
        "*italicized text*## Training CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKpCUhIT5EWp"
      },
      "source": [
        "if not usePytorch:\n",
        "    # Same code as above \n",
        "\n",
        "    epochs = 500 # number of iterations \n",
        "\n",
        "    if NNID>=2:\n",
        "        input_shape = [h,w,c] \n",
        "\n",
        "        if NNID==2:\n",
        "        model = getSimpleDNNModel(input_shape, number_of_pixels,number_of_classes)\n",
        "        elif NNID==3: # advanced \n",
        "        model = getDNNModel(input_shape, number_of_pixels,number_of_classes)\n",
        "        elif NNID==4: # 3D \n",
        "        input_shape = [h,w,c,1] \n",
        "        model = getSimpleDNN3DModel(input_shape, number_of_pixels,number_of_classes)\n",
        "\n",
        "        print(\"===================================================\")\n",
        "        print(\"               Training Loop           \")\n",
        "        print(\"===================================================\")\n",
        "        total_time_start = time.time()\n",
        "        # we loop number of iterations\n",
        "        # for each iteration, we loop through all the training samples\n",
        "        for epoch in range(epochs):\n",
        "            #print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Iterate over the batches of the dataset.\n",
        "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "                #print(train_dataset.shape)\n",
        "                #print(x_batch_train.shape,y_batch_train.shape)\n",
        "                if doAug: \n",
        "                    #do augmentation\n",
        "                    new_train_batch = doAugmentation(x_batch_train , y_batch_train , batch_size)                \n",
        "                    for stp, (new_x_batch_train, new_y_batch_train) in enumerate(new_train_batch):\n",
        "                        #print(stp)\n",
        "                        #print(new_train_batch.shape)\n",
        "                        #print(new_x_batch_train.shape,new_y_batch_train.shape)\n",
        "                        #model.summary()\n",
        "                        loss_value = train_step(model,new_x_batch_train, new_y_batch_train)\n",
        "                        train_acc = train_acc_metric.result()\n",
        "                        train_acc_metric.reset_states()\n",
        "                        print(\"   epoch:%d \\t stp %d trnLoss: %.4f \" % (epoch, stp, float(loss_value)))\n",
        "                else:                    \n",
        "                    loss_value = train_step(model,x_batch_train, y_batch_train)                    \n",
        "                    train_acc = train_acc_metric.result()\n",
        "                    train_acc_metric.reset_states()\n",
        "\n",
        "            # Run a validation loop at the end of each epoch.\n",
        "            for x_batch_val, y_batch_val in val_dataset:\n",
        "                val_loss_value = val_step(model,x_batch_val, y_batch_val)\n",
        "\n",
        "            val_acc = val_acc_metric.result()\n",
        "            val_acc_metric.reset_states()\n",
        "            \n",
        "            # compute time required for each epoch\n",
        "            end_time = time.time() - start_time\n",
        "\n",
        "            print(\"epoch:%d \\t trnLoss: %.4f \\t valLoss: %.4f \\t trnAcc: %.4f \\t valAcc: %.4f \\t time:  %.2f\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "            logFile = open(logFilePath,'a')\n",
        "            logFile.write(\"%d \\t %.4f \\t  %.4f \\t %.4f \\t  %.4f \\t  %.2f \\n\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "            logFile.close()\n",
        "            if epoch % 5 ==0:\n",
        "            # plot the result        \n",
        "            iaPlotLoss(logFilePath)\n",
        "            model.save(modelPath)      \n",
        "        # save the final model\n",
        "        model.save(modelPath)     \n",
        "\n",
        "        # plot the result        \n",
        "        iaPlotLoss(logFilePath)\n",
        "        total_time_end = time.time() - total_time_start\n",
        "        print(\"Training this dataset took \", total_time_end,\" seconds!\") \n",
        "        print(\"Training this dataset took \", total_time_end/60.0,\" minutes!\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK-U7PBvXGif"
      },
      "source": [
        "# py \n",
        "if usePytorch:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deIAKC2E5HcT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwUVTMeg5Ks5"
      },
      "source": [
        "if NNID==2:\n",
        "    # Load the saved model \n",
        "    model = keras.models.load_model(modelPath, compile=False)\n",
        "\n",
        "    start_time = time.time() \n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_tst, y_batch_tst in tst_dataset:\n",
        "        output = model.predict(x_batch_tst)\n",
        "        #y = keras.utils.to_categorical(y_batch_tst)\n",
        "        tst_acc_metric.update_state(y_batch_tst, output)\n",
        "\n",
        "    tst_acc = tst_acc_metric.result()\n",
        "\n",
        "    # compute time required for each epoch\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"test accuracy : %.4f \\t time:  %.2f\" % (  float(tst_acc), end_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-YaiYcJP028"
      },
      "source": [
        "#TODO: show examples using the above datasets\n",
        "# https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRdRNJF2kwU"
      },
      "source": [
        "# More resources:\n",
        "\n",
        "* 3Blue1Brown Neural Network [video tutorials](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) \n",
        "* Deep Learning Video Lectures by Prof. Andreas Maier [Winter 20/21](https://www.youtube.com/watch?v=SCFToE1vM2U&list=PLpOGQvPCDQzvJEPFUQ3mJz72GJ95jyZTh)\n",
        "* Some of the code in this notebook is taken from [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
        "* Calculating number of parameters in [CNN](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d)\n",
        "* Some of the code in this notebook is taken from [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb)\n",
        "* https://nihcc.app.box.com/v/ChestXray-NIHCC\n",
        "* https://www.tensorflow.org/datasets/catalog/patch_camelyon\n",
        "* https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/\n",
        "* https://elix-tech.github.io/ja/2016/06/02/kaggle-facial-keypoints-ja.html\n",
        "* https://fairyonice.github.io/achieving-top-23-in-kaggles-facial-keypoints-detection-with-keras-tensorflow.html\n",
        "* https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\n",
        "* https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb\n",
        "* https://github.com/nicknochnack/TFODCourse\n",
        "* https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/\n",
        "* https://github.com/enggen/Deep-Learning-Coursera\n",
        "* https://github.com/prateeshreddy/Deep-Learning-Coursera\n",
        "* https://github.com/JudasDie/deeplearning.ai\n",
        "* https://github.com/aladdinpersson/Machine-Learning-Collection/tree/master/ML/Pytorch/object_detection/YOLO\n",
        "* https://youtu.be/n9_XyCGr-MI"
      ]
    }
  ]
}