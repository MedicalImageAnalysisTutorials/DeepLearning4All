{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA_DNN_ImageObjectDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOwObP1k9e96xRa8gpZ71ud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedicalImageAnalysisTutorials/DeepLearning4All/blob/main/IA_DNN_ImageObjectDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figzmLKBxWC8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Draft version**\n",
        "In this notebook, I will try to provide a practical tutorial for deep learning using simple examples. I will try to use simple implementation and avoid using built-in functions to give clear idea about the concept. You need basic programming knowledge. \n",
        "\n",
        "**The Object Detection Problem:**\n",
        "\n",
        "**The Object Recognition Problem:**\n",
        "                 .\n",
        "I made the code flexible so one can try different approaches, datsaets, optimisers, loss functions based on if else statements. \n",
        "\n",
        "The general code structure:\n",
        "\n",
        "    1. Read and pre-process the data\n",
        "    2. Create the model, optimiser, loss function, and metrics\n",
        "    3. Start the training loop\n",
        "    4. Evaluate the final model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooQ5tm85ZDb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "outputId": "6eb4ba74-84f9-4c74-dacc-d1329bec14fe"
      },
      "source": [
        "# TODO:\n",
        "# complete the introduction\n",
        "# add the introduction with the figures to this notebook https://github.com/idhamari/Deep-Learning-Coursera/blob/master/Convolutional%20Neural%20Networks/Week3/Car%20detection%20for%20Autonomous%20Driving/Autonomous_driving_application_Car_detection_v3a.ipynb\n",
        "# define yolo model and its related functions \n",
        "# find dataset and use it \n",
        "\n",
        "import sys\n",
        "# Setup \n",
        "doInstall =1\n",
        "if doInstall:\n",
        "    # !git clone https://github.com/JudasDie/deeplearning.ai/blob/master/Convolutional%20Neural%20Networks/week3/yolo_utils.py\n",
        "    # from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
        "    #!git clone https://github.com/allanzelener/yad2k.git\n",
        "    !git  clone https://github.com/idhamari/YAD2K.git\n",
        "    !pip install SimpleITK\n",
        "    !pip install labelImg \n",
        "#   !git clone https://github.com/tensorflow/models.git \n",
        "#   !wget https://github.com/protocolbuffers/protobuf/releases/download/v3.18.0/protoc-3.18.0-linux-x86_64.zip\n",
        "#   !unzip protoc-3.18.0-linux-x86_64.zip -d protoc-3.18.0\n",
        "  #sys.path.append('/protoc-3.18.0/bin')\n",
        "#   !export PATH=\"protoc-3.18.0/bin:$PATH\"\n",
        "#   !cd models/research\n",
        "#   !protoc object_detection/protos/*.proto --python_out=.\n",
        "#   !cd ../../cocoapi/PythonAPI\n",
        "#   !git clone https://github.com/cocodataset/cocoapi.git\n",
        "#   !make\n",
        "#   !cp -r pycocotools <PATH_TO_TF>/TensorFlow/models/research/\n",
        "    # !cd models/research\n",
        "    # !cp object_detection/packages/tf2/setup.py .\n",
        "    # !python -m pip install --use-feature=2020-resolver .\n",
        "    # !mkdir workspace\n",
        "    # !mkdir workspace/training_demo\n",
        "    # #store all *.csv files and the respective TensorFlow *.record files, which contain the list of annotations for our dataset images.\n",
        "    # !mkdir workspace/training_demo/annotations\n",
        "    # #store exported versions of our trained model(s).\n",
        "    # !mkdir workspace/training_demo/exported-models\n",
        "    # #contain a sub-folder for each of training job. Each subfolder will contain the\n",
        "    # # training pipeline configuration file *.config, as well as all files generated during the training and evaluation of our model.\n",
        "    # !mkdir workspace/training_demo/models\n",
        "    # !mkdir workspace/training_demo/pre-trained-models\n",
        "    # #contains a copy of all the images in our dataset, as well as the respective *.xml files produced for each one, \n",
        "    # # once labelImg is used to annotate objects\n",
        "    # !mkdir workspace/training_demo/images\n",
        "    # !mkdir workspace/training_demo/images/train\n",
        "    # !mkdir workspace/training_demo/images/test\n",
        "import os, random, time, math, colorsys, imghdr, PIL\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.misc\n",
        "import pandas as pd\n",
        "\n",
        "from skimage.transform import resize\n",
        "import cv2 \n",
        "import SimpleITK as sitk \n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, Lambda, Conv2D, BatchNormalization\n",
        "from keras.models import load_model, Model\n",
        "\n",
        "\n",
        "# Yolo\n",
        "from yad2k.yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# to reproduce the same results given same input\n",
        "np.random.seed(1)               \n",
        "!ls\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YAD2K'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (243/243), 2.35 MiB | 11.63 MiB/s, done.\n",
            "Resolving deltas: 100% (106/106), done.\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 4.7 kB/s \n",
            "\u001b[?25hInstalling collected packages: SimpleITK\n",
            "Successfully installed SimpleITK-2.1.1\n",
            "Collecting labelImg\n",
            "  Downloading labelImg-1.8.5.tar.gz (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting pyqt5\n",
            "  Downloading PyQt5-5.15.4-cp36.cp37.cp38.cp39-abi3-manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.3 MB 29.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from labelImg) (4.2.6)\n",
            "Collecting PyQt5-sip<13,>=12.8\n",
            "  Downloading PyQt5_sip-12.9.0-cp37-cp37m-manylinux1_x86_64.whl (317 kB)\n",
            "\u001b[K     |████████████████████████████████| 317 kB 49.2 MB/s \n",
            "\u001b[?25hCollecting PyQt5-Qt5>=5.15\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.9 MB 35 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: labelImg\n",
            "  Building wheel for labelImg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for labelImg: filename=labelImg-1.8.5-py2.py3-none-any.whl size=258444 sha256=67856c82403a41b7365d85454aa56cfeeb20e176fb37fcbf0bf89eca2a71f48b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/fc/d0/41e6a87f4360fa551173280bd3c871e48711436e7873c6929c\n",
            "Successfully built labelImg\n",
            "Installing collected packages: PyQt5-sip, PyQt5-Qt5, pyqt5, labelImg\n",
            "Successfully installed PyQt5-Qt5-5.15.2 PyQt5-sip-12.9.0 labelImg-1.8.5 pyqt5-5.15.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b59a04fbd52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# Yolo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myad2k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myad2k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_yolo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myolo_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_boxes_to_corners\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_true_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolo_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yad2k'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WAn3WH3Lpr"
      },
      "source": [
        "# Image Object Detection using CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTv78FC4P_J"
      },
      "source": [
        "## Reading and exploring the datasets\n",
        "\n",
        "We are using Face Detection Data Set and Benchmark (FDDB) [dataset](http://vis-www.cs.umass.edu/fddb/README.txt) from university of Massachusetts.  \n",
        "\n",
        "\n",
        "FDDB-folds contains files with names: FDDB-fold-xx.txt and FDDB-fold-xx-ellipseList.txt, where xx = {01, 02, ..., 10} represents the fold-index.\n",
        "\n",
        "Each line in the \"FDDB-fold-xx.txt\" file specifies a path to an image in the above-mentioned data set. For instance, the entry \"2002/07/19/big/img_130\" corresponds to \"originalPics/2002/07/19/big/img_130.jpg.\"\n",
        "\n",
        "The corresponding annotations are included in the file \"FDDB-fold-xx-ellipseList.txt\" in the following \n",
        "format:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "<image name i>\n",
        "<number of faces in this image =im>\n",
        "<face i1>\n",
        "<face i2>\n",
        "...\n",
        "<face im>\n",
        "```\n",
        "\n",
        "Here, each face is denoted by:\n",
        "\n",
        "            major_axis_radius minor_axis_radius angle center_x center_y 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzj-Tvek4V8s"
      },
      "source": [
        "datasetID         = 2  # 1:minst is selected by default, for cifar10 use 2\n",
        "NNID              = 4  # 1:NN is by default, for DNN use 2,or 3, for 3D use 4  \n",
        "number_of_classes = 10  # each datasets have 10 classes\n",
        "showSamples = 1\n",
        "doDownload = 0\n",
        "# if you have large GPU memory you can combine the images to batches \n",
        "# for faster training.\n",
        "# It is good to try different values\n",
        "batch_size = 2 # you can try larger batch size e.g. 1024 * 6\n",
        "\n",
        "# Facial dataset\n",
        "# more info http://vis-www.cs.umass.edu/fddb/README.txt\n",
        "\n",
        "if doDownload:\n",
        "    !wget http://vis-www.cs.umass.edu/fddb/originalPics.tar.gz\n",
        "    !tar zxvf  originalPics.tar.gz\n",
        "\n",
        "    !wget http://vis-www.cs.umass.edu/fddb/FDDB-folds.tgz\n",
        "    !tar zxvf FDDB-folds.tgz\n",
        "    !mkdir originalPics\n",
        "    !mv 2002 originalPics\n",
        "    !mv 2003 originalPics\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# class_names = range(10)\n",
        "\n",
        "# if datasetID==2:\n",
        "#     # cifar10 dataset\n",
        "#     # The CIFAR10 dataset contains 60,000 color images in 10 classes, \n",
        "#     # with 6,000 images in each class.\n",
        "#     # The dataset is divided into 50,000 training images and 10,000 testing images.\n",
        "#     # The classes are mutually exclusive and there is no overlap between them.\n",
        "\n",
        "#     (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "#     class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#                'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "#     if NNID==4:\n",
        "#         #TODO: fix this \n",
        "#         showSamples =0\n",
        "        \n",
        "#         x_train = x_train.reshape(-1,32*32*3) # (32 * 32 * 3)        \n",
        "#         x_train = np.resize(x_train,(x_train.shape[0],15,15,15))        \n",
        "#         x_train = x_train.reshape(-1,15,15,15)\n",
        "#         x_test = x_test.reshape(-1,32*32*3) # (32 * 32 * 3)\n",
        "#         x_test = np.resize(x_test,(x_test.shape[0],15,15,15))\n",
        "#         x_test = x_test.reshape(-1,15,15,15)\n",
        "#         x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "#         y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "#         x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "#         y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "#         print(x_train.shape)\n",
        "#         print(x_test.shape)\n",
        "\n",
        "\n",
        "# # get size \n",
        "# h = x_train.shape[1] # image height\n",
        "# w = x_train.shape[2] # image width\n",
        "# # check for rgb \n",
        "# try:\n",
        "#     # number of channels\n",
        "#     c =  x_train.shape[3]\n",
        "# except:\n",
        "#     # number of channels\n",
        "#     c =  1\n",
        "#     # if there is no number of channels, add 1\n",
        "#     x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "#     y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "#     x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "#     y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "# # Reserve 10,000 samples for validation.\n",
        "# x_val = x_train[-10000:]\n",
        "# y_val = y_train[-10000:]\n",
        "# x_train = x_train[:-10000]\n",
        "# y_train = y_train[:-10000]\n",
        "\n",
        "# number_of_pixels = h * w * c\n",
        "\n",
        "\n",
        "# print(\"dataset shape   : \",x_train.shape)\n",
        "# print(\"number of images: \",x_train.shape[0])\n",
        "# print(\"image size      : \",x_train[0].shape)\n",
        "# print(\"image data type : \",type(x_train[0][0][0][0]))\n",
        "# print(\"image max  value: \",np.max(x_train[0]))\n",
        "# print(\"image min  value: \",np.min(x_train[0]))\n",
        "# if c==1:\n",
        "#    print(\"gray or binary image (not color image)\")\n",
        "# elif c==3:\n",
        "#    print(\"rgb color image (or probably non-color image represented with 3 channels)\")\n",
        "\n",
        "\n",
        "# # display sample images \n",
        "# if showSamples:\n",
        "#     plt.figure(figsize=(10,10))\n",
        "#     for i in range(25):\n",
        "#         plt.subplot(5,5,i+1)\n",
        "#         plt.xticks([])\n",
        "#         plt.yticks([])\n",
        "#         plt.grid(False)\n",
        "#         #plt.imshow(x_train[i])\n",
        "#         plt.imshow(cv2.cvtColor(x_train[i], cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#         # The CIFAR labels happen to be arrays, \n",
        "#         # which is why you need the extra index\n",
        "#         if datasetID==1:\n",
        "#             plt.xlabel(y_train[i])\n",
        "#         elif datasetID==2:\n",
        "#             plt.xlabel(class_names[y_train[i][0]])\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# # normalisation\n",
        "# x_train = np.array([ x/255.0 for x in x_train])\n",
        "# x_val   = np.array([ x/255.0 for x in x_val])\n",
        "# x_test  = np.array([ x/255.0 for x in x_test])\n",
        "# #y_train = y_train.astype(np.float32)\n",
        "\n",
        "# # for NN we need 1D \n",
        "# if NNID ==1:\n",
        "#    x_train = np.reshape(x_train, (-1, number_of_pixels))\n",
        "#    x_val   = np.reshape(x_val,  (-1, number_of_pixels))\n",
        "#    x_test  = np.reshape(x_test , (-1, number_of_pixels))\n",
        "\n",
        "# # Prepare the training dataset.\n",
        "# print(x_train.shape,y_train.shape)\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "# #train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "\n",
        "# # Prepare the validation dataset.\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "# val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# # Prepare the test dataset.\n",
        "# tst_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "# tst_dataset = tst_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIDkZloTgOkA"
      },
      "source": [
        "import os \n",
        "from glob import glob\n",
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# class_names = range(10)\n",
        "trnRatio = 0.90\n",
        "tstRatio = 1 - trnRatio\n",
        "img_fnms = sorted([y for x in os.walk(\"originalPics\") for y in glob(os.path.join(x[0], '*.jpg'))])\n",
        "lbls_fnms = sorted([y for x in os.walk(\"FDDB-folds\") for y in glob(os.path.join(x[0], '*ellipseList.txt'))])\n",
        "print((img_fnms[0:2]))\n",
        "print(len(img_fnms))\n",
        "print((lbls_fnms[0:2]))\n",
        "print(len(lbls_fnms))\n",
        "print(\"--------------------------\")\n",
        "lblInfo = []\n",
        "numObj = 0\n",
        "for fnm in lbls_fnms:\n",
        "    f = open(fnm,'r')\n",
        "    lines = f.readlines()\n",
        "    imgLoc=[]\n",
        "    for x in lines:\n",
        "        if \"img\" in x: \n",
        "            imgPath = x.strip()\n",
        "        elif \".\" in x:     \n",
        "            imgLoc.append([float(y) for y in x.split()])\n",
        "            if len(imgLoc) == numObj:\n",
        "              lblInfo.append([imgPath,numObj,imgLoc,])                   \n",
        "              imgLoc=[]\n",
        "              numObj=0\n",
        "        else:\n",
        "            numObj = int(x)\n",
        "\n",
        "print(\"---------------------------\")\n",
        "# annotations for 5171 faces\n",
        "# 2845 images  ???\n",
        "print(len(lblInfo))\n",
        "s = 0 \n",
        "for x in lblInfo:\n",
        "#     print(x[0])\n",
        "#     print(x[1])\n",
        "      s = s  + x[1]\n",
        "#     for i in range (x[1]):\n",
        "#         print(x[2][i])\n",
        "print(s)\n",
        "# imgLst = [1             , 2               ]\n",
        "# lblLst = [[1,3,[loc]]   , [1,3,[loc]]     ]\n",
        "\n",
        "#(x_train, y_train), (x_test, y_test)\n",
        "# class_names = range(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP0E_Pzs04y_"
      },
      "source": [
        "## Dataset augmentation\n",
        "\n",
        "It is important to train the model on different variations of the dataset. It is also important to have large datset for training.\n",
        "\n",
        "Using dataset augmentation helps to achieve both of the above goals. From one image, one can generate hundred thousands of images using image transformation.\n",
        "\n",
        "The image transformation could be [spatial transform]() or point transform where we move the points of the image to new locations e.g. shifting, flipping, and/or rotating the imag. \n",
        "\n",
        "Another type of transformation is intensity transform or pixel transform where we change the color values of the pixels in the image e.g. invert the color, add more brightness or darkness. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb2KdoTR07L2"
      },
      "source": [
        "# TODO\n",
        "doAug = 1\n",
        "\n",
        "def imagePixelTransforms(img):    \n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    img1   = 1.0- img # invert color\n",
        "    img2   = img +0.3 # more brightness\n",
        "    img3   = img -0.3 # more darkness\n",
        "    images = np.array([img1,img2,img3])\n",
        "    images = [ img.reshape(img.shape) for img in images]\n",
        "\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    return images\n",
        "\n",
        "def imagePointTransforms(img):\n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    # Perform the rotation\n",
        "    center  = (img.shape[0] / 2, img.shape[1] / 2)\n",
        "    sz      = (img.shape[1], img.shape[0])\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 45, 1)\n",
        "    img1 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img1 = img1[...,np.newaxis] if img1.shape !=img.shape else img1\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 90, 1)\n",
        "    img2 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img2 = img2[...,np.newaxis] if img2.shape !=img.shape else img2\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 270, 1)\n",
        "    img3 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img3 = img3[...,np.newaxis] if img3.shape !=img.shape else img3\n",
        "\n",
        "    images = np.array([img1,img2,img3])\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    #plt.figure() ;    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # plt.figure() ;    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    return images\n",
        "\n",
        "\n",
        "# define a function for sitk transform\n",
        "def resample(img_array, transform):\n",
        "    # Output image Origin, Spacing, Size, Direction are taken from the reference\n",
        "    # image in this call to Resample\n",
        "    image = sitk.GetImageFromArray(img_array)\n",
        "    reference_image = image\n",
        "    interpolator = sitk.sitkCosineWindowedSinc\n",
        "    default_value = 100.0\n",
        "    resampled_img = sitk.Resample(image, reference_image, transform,\n",
        "                         interpolator, default_value)\n",
        "    resampled_array = sitk.GetArrayFromImage(resampled_img)\n",
        "    return resampled_array\n",
        "\n",
        "def affine_rotate(transform, degrees):\n",
        "    parameters = np.array(transform.GetParameters())\n",
        "    new_transform = sitk.AffineTransform(transform)\n",
        "    dimension =3 \n",
        "    matrix = np.array(transform.GetMatrix()).reshape((dimension,dimension))\n",
        "    radians = -np.pi * degrees / 180.\n",
        "    rotation = np.array([[1  ,0,0], \n",
        "                         [0, np.cos(radians), -np.sin(radians)],\n",
        "                         [0, np.sin(radians), np.cos(radians)]]\n",
        "                        )\n",
        "    new_matrix = np.dot(rotation, matrix)\n",
        "    new_transform.SetMatrix(new_matrix.ravel())\n",
        "    return new_transform\n",
        "\n",
        "\n",
        "def imagePoint3DTransforms(img):\n",
        "    #print(\"imagePoint3DTransforms\")\n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    # Perform the rotation\n",
        "    # In SimpleITK resampling convention, the transformation maps points \n",
        "    # from the fixed image to the moving image,\n",
        "    # so inverse of the transform is applied\n",
        "\n",
        "    center = (img.shape[0] /2, img.shape[1] /2,img.shape[1] /2)\n",
        "    rotation_around_center = sitk.AffineTransform(3)\n",
        "    rotation_around_center.SetCenter(center)\n",
        "    \n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -45)\n",
        "    img1 = resample(img, rotation_around_center)\n",
        "\n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -90)\n",
        "    img2 = resample(img, rotation_around_center)\n",
        "\n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -90)\n",
        "    img3 = resample(img, rotation_around_center)\n",
        "\n",
        "    images = np.array([img1,img2,img3])\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    #plt.figure() ;    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # plt.figure() ;    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    return images\n",
        "\n",
        "def doAugmentation(images,labels,batch_size):\n",
        "    # input is an image or a batch e.g. list of images \n",
        "    # get numpy arrays from the tensor    \n",
        "    images = images.numpy()\n",
        "    labels = labels.numpy()\n",
        "    # if 1d convert back to 2d\n",
        "    #print(images.shape)\n",
        "    rgb = 0 ; is3d = 0\n",
        "    if len(images.shape) == 2:\n",
        "       try: \n",
        "          img2d_shape = int(math.sqrt(images.shape[1])) # gray or binary image\n",
        "          images =images.reshape(-1,img2d_shape,img2d_shape)\n",
        "       except:\n",
        "          try: \n",
        "            img2d_shape = int(math.sqrt(images.shape[1]/3)) # rgb image\n",
        "            images =images.reshape(-1,img2d_shape,img2d_shape,3)\n",
        "            rgb = 1  \n",
        "          except:\n",
        "            pass  \n",
        "            # img3d_shape = int(math.sqrt(images.shape[1]/3)) # rgb image\n",
        "            # images =images.reshape(-1,img2d_shape,img2d_shape,3)\n",
        "            # is3d = 1  \n",
        "\n",
        "\n",
        "\n",
        "    x_outputs = [] ; y_outputs = []\n",
        "    i = 0\n",
        "    for img in images:\n",
        "        #print(\"-------------------------\", i ,\"--------------------\")\n",
        "        if NNID==4:\n",
        "           img = img.squeeze() \n",
        "        # from each images we generate 6 images\n",
        "        # 64 batch will generate 448\n",
        "        x_outputs.extend([img])\n",
        "        imgs1 = imagePoint3DTransforms(img)\n",
        "        imgs2 = imagePixelTransforms(img)\n",
        "        #if not rgb:\n",
        "           #imgs1 = np.array( x[...,np.newaxis] for x in imgs1 if len(x.shape)<3) \n",
        "           #imgs2 = np.array( x[...,np.newaxis] for x in imgs2 if len(x.shape)<3)\n",
        "        x_outputs.extend(imgs1) # 3 images\n",
        "        x_outputs.extend(imgs2) # 3 images\n",
        "        # print(img.shape)\n",
        "        # print(imgs1[0].shape)\n",
        "        # print(imgs2[0].shape)\n",
        "        # assign the same label to all transformed images\n",
        "        for j in range ( len(imgs1) +len(imgs2)+1):\n",
        "            y_outputs.extend([labels[i]])\n",
        "\n",
        "        i = i +1\n",
        "    x_outputs = np.array(x_outputs)\n",
        "    if NNID==4:\n",
        "       x_outputs = np.array([x[...,np.newaxis] for x in x_outputs])\n",
        "    y_outputs = np.array(y_outputs)\n",
        "\n",
        "    if (not rgb) and (NNID==1):\n",
        "       x_outputs = np.reshape(x_outputs, (-1,img2d_shape*img2d_shape,1))\n",
        "    elif (rgb) and (NNID==1):\n",
        "       x_outputs = np.reshape(x_outputs, (-1,img2d_shape*img2d_shape*3))   \n",
        "\n",
        "    new_train_dataset = tf.data.Dataset.from_tensor_slices((x_outputs, y_outputs))\n",
        "    new_train_dataset = new_train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    return new_train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmSF08Oq4ae6"
      },
      "source": [
        "# NN TensorFlow\n",
        "def getNNModel(number_of_pixels,number_of_classes):\n",
        "    inputs = keras.Input(shape=(number_of_pixels,), name=\"digits\")\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
        "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "print(\"NN model is defined ...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpxH1ntsqfuZ"
      },
      "source": [
        "# TODO\n",
        "# NN pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtEO2ORyBFeu"
      },
      "source": [
        "## Define optimiser and loss function for NN and CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2CBVAaABKU4"
      },
      "source": [
        "# Instantiate an optimizer to train the model.\n",
        "\n",
        "optimiserID = 1 # SGD by default for ADAM use 2 \n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "if optimiserID ==2:\n",
        "   optimizer = keras.optimizers.Adam()#learning_rate=0.0001\n",
        "# Instantiate a loss function.\n",
        "\n",
        "lossFunctionID = 1 # SparseCategoricalCrossentropy by default for MSE use 2 \n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "val_acc_metric   = keras.metrics.SparseCategoricalAccuracy()\n",
        "tst_acc_metric   = keras.metrics.SparseCategoricalAccuracy()\n",
        "if lossFunctionID==2:\n",
        "   loss_fn = keras.losses.MeanSquaredError()\n",
        "   # Prepare the metrics.\n",
        "   train_acc_metric = keras.metrics.MeanSquaredError()\n",
        "   val_acc_metric   = keras.metrics.MeanSquaredError()\n",
        "   tst_acc_metric   = keras.metrics.MeanSquaredError()\n",
        "\n",
        "elif  lossFunctionID==3:\n",
        "   loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "   # Prepare the metrics.\n",
        "   train_acc_metric = keras.metrics.CategoricalCrossentropy()\n",
        "   val_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "   tst_acc_metric   = keras.metrics.CategoricalCrossentropy()\n",
        "print(\"optimiser, loss, and metrics are defined .... \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmAbPxuDqrVV"
      },
      "source": [
        "# TODO\n",
        "# Pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxyj5W7P4d2M"
      },
      "source": [
        "\n",
        "## Define required functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8dFra_t4i8d"
      },
      "source": [
        "\n",
        "# define training parameters and file paths \n",
        "\n",
        "# model log files path\n",
        "modelPath   = \"./modelClassification.h5\"\n",
        "logFilePath = \"./training_log.csv\"\n",
        "figPath     = \"./training_log.png\"\n",
        "\n",
        "logFile = open(logFilePath,'w')\n",
        "logFile.write(\"epoch \\t trnLoss \\t valLoss \\t trnAcc \\t valAcc \\t time \\n\" )\n",
        "logFile.close()\n",
        "# Using optimised tensorflow functions provides more speed\n",
        "\n",
        "@tf.function\n",
        "def train_step(model,x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        #print(\"get result\")\n",
        "        model_output = model(x, training=True)\n",
        "        #print(\"get loss value \")\n",
        "        #y = keras.utils.to_categorical(y)\n",
        "        loss_value = loss_fn(y, model_output)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "\n",
        "@tf.function\n",
        "def val_step(model,x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    #y = keras.utils.to_categorical(y)\n",
        "    loss_value = loss_fn(y, val_logits)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "    return loss_value\n",
        "\n",
        "# plotting function to monitor the curves\n",
        "def iaPlotLoss(logPath,figPath=None):\n",
        "    f = open(logPath,'r')\n",
        "    lst = f.readlines()\n",
        "    # first line is labels:\n",
        "    labels = lst[0].split()[1:-2]\n",
        "    x  = [ int(  ln.split()[0]) for ln in lst[1:]] # epoch\n",
        "    y1 = [ float(ln.split()[1]) for ln in lst[1:]] # lossTrain\n",
        "    y2 = [ float(ln.split()[2]) for ln in lst[1:]] # lossValidation\n",
        "    y3 = [ float(ln.split()[3]) for ln in lst[1:]] # accTrain\n",
        "    y4 = [ float(ln.split()[4]) for ln in lst[1:]] # accValidation\n",
        "    #plotting    \n",
        "    plt.clf()\n",
        "    fig, ax = plt.subplots()    \n",
        "    l1, = ax.plot(x, y1) ;     l2, = ax.plot(x, y2) ;\n",
        "    l3, = ax.plot(x, y3) ;     l4, = ax.plot(x, y4) ;\n",
        "    ax.legend((l1, l2,l3,l4), labels, loc='upper right', shadow=True)\n",
        "    plt.xlabel('epoch')\n",
        "    if figPath:\n",
        "        plt.savefig(figPath, bbox_inches='tight')\n",
        "    else:\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "#================================================\n",
        "#                  YOLO\n",
        "#================================================\n",
        "def read_classes(classes_path):\n",
        "    with open(classes_path) as f:\n",
        "        class_names = f.readlines()\n",
        "    class_names = [c.strip() for c in class_names]\n",
        "    return class_names\n",
        "\n",
        "def read_anchors(anchors_path):\n",
        "    with open(anchors_path) as f:\n",
        "        anchors = f.readline()\n",
        "        anchors = [float(x) for x in anchors.split(',')]\n",
        "        anchors = np.array(anchors).reshape(-1, 2)\n",
        "    return anchors\n",
        "\n",
        "def generate_colors(class_names):\n",
        "    hsv_tuples = [(x / len(class_names), 1., 1.) for x in range(len(class_names))]\n",
        "    colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
        "    colors = list(map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)), colors))\n",
        "    random.seed(10101)      # Fixed seed for consistent colors across runs.\n",
        "    random.shuffle(colors)  # Shuffle colors to decorrelate adjacent classes.\n",
        "    random.seed(None)       # Reset seed to default.\n",
        "    return colors\n",
        "\n",
        "def scale_boxes(boxes, image_shape):\n",
        "    \"\"\" Scales the predicted boxes in order to be drawable on the image\"\"\"\n",
        "    height = image_shape[0]\n",
        "    width = image_shape[1]\n",
        "    image_dims = K.stack([height, width, height, width])\n",
        "    image_dims = K.reshape(image_dims, [1, 4])\n",
        "    boxes = boxes * image_dims\n",
        "    return boxes\n",
        "\n",
        "def preprocess_image(img_path, model_image_size):\n",
        "    image_type = imghdr.what(img_path)\n",
        "    image = Image.open(img_path)\n",
        "    resized_image = image.resize(tuple(reversed(model_image_size)), Image.BICUBIC)\n",
        "    image_data = np.array(resized_image, dtype='float32')\n",
        "    image_data /= 255.\n",
        "    image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
        "    return image, image_data\n",
        "\n",
        "def draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors):   \n",
        "    font = ImageFont.truetype(font='font/FiraMono-Medium.otf',size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
        "    thickness = (image.size[0] + image.size[1]) // 300\n",
        "\n",
        "    for i, c in reversed(list(enumerate(out_classes))):\n",
        "        predicted_class = class_names[c]\n",
        "        box = out_boxes[i]\n",
        "        score = out_scores[i]\n",
        "\n",
        "        label = '{} {:.2f}'.format(predicted_class, score)\n",
        "\n",
        "        draw = ImageDraw.Draw(image)\n",
        "        label_size = draw.textsize(label, font)\n",
        "\n",
        "        top, left, bottom, right = box\n",
        "        top = max(0, np.floor(top + 0.5).astype('int32'))\n",
        "        left = max(0, np.floor(left + 0.5).astype('int32'))\n",
        "        bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
        "        right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
        "        print(label, (left, top), (right, bottom))\n",
        "\n",
        "        if top - label_size[1] >= 0:\n",
        "            text_origin = np.array([left, top - label_size[1]])\n",
        "        else:\n",
        "            text_origin = np.array([left, top + 1])\n",
        "\n",
        "        # My kingdom for a good redistributable image drawing library.\n",
        "        for i in range(thickness):\n",
        "            draw.rectangle([left + i, top + i, right - i, bottom - i], outline=colors[c])\n",
        "        draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill=colors[c])\n",
        "        draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
        "        del draw\n",
        "\n",
        "\n",
        "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
        "    box_scores = box_confidence*box_class_probs\n",
        "    box_classes = K.argmax(box_scores,-1)\n",
        "    box_class_scores = K.max(box_scores,-1)\n",
        "    filtering_mask = box_class_scores>threshold\n",
        "    scores = tf.boolean_mask(box_class_scores,filtering_mask)\n",
        "    boxes = tf.boolean_mask(boxes,filtering_mask)\n",
        "    classes = tf.boolean_mask(box_classes,filtering_mask)\n",
        " \n",
        "    return scores, boxes, classes      \n",
        "\n",
        "# Intersection over Union\n",
        "# check if predicted box is good iou>0.5\n",
        "def iou(box1, box2):\n",
        "    xi1 = max(box1[0],box2[0])\n",
        "    yi1 = max(box1[1],box2[1])\n",
        "    xi2 = min(box1[2],box2[2])\n",
        "    yi2 = min(box1[3],box2[3])\n",
        "    inter_area = (yi2-yi1)*(xi2-xi1)\n",
        "    box1_area = (box1[3]-box1[1])*(box1[2]-box1[0])\n",
        "    box2_area = (box2[3]-box2[1])*(box2[2]-box2[0])\n",
        "    union_area = box1_area+box2_area-inter_area\n",
        "    iou = inter_area/union_area\n",
        "    return iou  \n",
        "\n",
        "# find the best box from number of detcted boxes\n",
        "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
        "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n",
        "    K.get_session().run(tf.variables_initializer([max_boxes_tensor]))\n",
        "    nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes,iou_threshold)\n",
        "    scores = K.gather(scores,nms_indices)\n",
        "    boxes = K.gather(boxes,nms_indices)\n",
        "    classes = K.gather(classes,nms_indices)\n",
        "    return scores, boxes, classes    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mox2km11qyqY"
      },
      "source": [
        "##TODO pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlx1hVMQGAbR"
      },
      "source": [
        "if NNID==1:\n",
        "    # Load the saved model \n",
        "    model = keras.models.load_model(modelPath, compile=False)\n",
        "\n",
        "    start_time = time.time() \n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_tst, y_batch_tst in tst_dataset:\n",
        "        output = model.predict(x_batch_tst)\n",
        "        #y = keras.utils.to_categorical(y_batch_tst)\n",
        "        tst_acc_metric.update_state(y_batch_tst, output)\n",
        "\n",
        "    tst_acc = tst_acc_metric.result()\n",
        "\n",
        "    # compute time required for each epoch\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"test accuracy : %.4f \\t time:  %.2f\" % (  float(tst_acc), end_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZmXOUfrDho"
      },
      "source": [
        "# TODO Pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPkWBAJP47f1"
      },
      "source": [
        "## Creating CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2a1zuKu4_ZR"
      },
      "source": [
        "# Simple DNN\n",
        "# just two conolution layers followed by dense layer\n",
        "def getSimpleDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 16 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv2D(nF, (3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x13  = layers.MaxPooling2D((2, 2)) (x11)\n",
        "    x21  = layers.Conv2D(2*nF, (3, 3), activation='relu') (x13)\n",
        "    x23  = layers.MaxPooling2D((2, 2))(x21)\n",
        "    #dense layer for classification\n",
        "    x31 = layers.Flatten()(x23)# convert from 3d to 1d\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x31)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# this is a better model for CIFAR10\n",
        "def getDNNModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 64 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv2D(nF, (3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x12  = layers.BatchNormalization()(x11)\n",
        "    x13  = layers.MaxPooling2D((2, 2)) (x12)\n",
        "    x14  = layers.Dropout(0.25)(x13)\n",
        "    x21  = layers.Conv2D(2*nF, (3, 3), activation='relu') (x14)\n",
        "    x22  = layers.BatchNormalization()(x21)\n",
        "    x23  = layers.MaxPooling2D((2, 2))(x22)\n",
        "    x24  = layers.Dropout(0.25)(x23)\n",
        "    x31  = layers.Conv2D(2*nF, (3, 3), activation='relu')(x24)\n",
        "    #dense layer for classification\n",
        "    x41 = layers.Flatten()(x31)# convert from 3d to 1d\n",
        "    #x7 = layers.Dense(2*nF, activation='relu')(x6)\n",
        "    #x8 = layers.Dense(2*nF, activation='relu')(x7)\n",
        "    x42  = layers.Dropout(0.50)(x41)\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x42)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "def getSimpleDNN3DModel(input_shape,number_of_pixels,number_of_classes):\n",
        "    nF        = 16 # number of filters\n",
        "    inputs = keras.Input(shape=input_shape, name=\"images\") \n",
        "    # Create CNN model\n",
        "    x11  = layers.Conv3D(nF, (3, 3, 3), activation='relu', input_shape=input_shape) (inputs)\n",
        "    x13  = layers.MaxPooling3D((2, 2, 2)) (x11)\n",
        "    x21  = layers.Conv3D(2*nF, (3, 3, 3), activation='relu') (x13)\n",
        "    x23  = layers.MaxPooling3D((2, 2 ,2))(x21)\n",
        "    #dense layer for classification\n",
        "    x31 = layers.Flatten()(x23)# convert from 3d to 1d\n",
        "    outputs = layers.Dense(number_of_classes, name=\"predictions\")(x31)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "#========================================\n",
        "#              Yolo  \n",
        "#========================================\n",
        "\n",
        "\n",
        "def getYoloDNNModel:\n",
        "    model = 0\n",
        "\n",
        "    return model    \n",
        "\n",
        "\n",
        "print(\"DNN model is defined ...\")    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_BI-0C4_kr"
      },
      "source": [
        "## Training CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKpCUhIT5EWp"
      },
      "source": [
        "# Same code as above \n",
        "\n",
        "epochs = 500 # number of iterations \n",
        "\n",
        "if NNID>=2:\n",
        "    input_shape = [h,w,c] \n",
        "\n",
        "    if NNID==2:\n",
        "       model = getSimpleDNNModel(input_shape, number_of_pixels,number_of_classes)\n",
        "    elif NNID==3: # advanced \n",
        "       model = getDNNModel(input_shape, number_of_pixels,number_of_classes)\n",
        "    elif NNID==4: # 3D \n",
        "       input_shape = [h,w,c,1] \n",
        "       model = getSimpleDNN3DModel(input_shape, number_of_pixels,number_of_classes)\n",
        "\n",
        "    print(\"===================================================\")\n",
        "    print(\"               Training Loop           \")\n",
        "    print(\"===================================================\")\n",
        "    total_time_start = time.time()\n",
        "    # we loop number of iterations\n",
        "    # for each iteration, we loop through all the training samples\n",
        "    for epoch in range(epochs):\n",
        "        #print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Iterate over the batches of the dataset.\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "            #print(train_dataset.shape)\n",
        "            #print(x_batch_train.shape,y_batch_train.shape)\n",
        "            if doAug: \n",
        "                #do augmentation\n",
        "                new_train_batch = doAugmentation(x_batch_train , y_batch_train , batch_size)                \n",
        "                for stp, (new_x_batch_train, new_y_batch_train) in enumerate(new_train_batch):\n",
        "                    #print(stp)\n",
        "                    #print(new_train_batch.shape)\n",
        "                    #print(new_x_batch_train.shape,new_y_batch_train.shape)\n",
        "                    #model.summary()\n",
        "                    loss_value = train_step(model,new_x_batch_train, new_y_batch_train)\n",
        "                    train_acc = train_acc_metric.result()\n",
        "                    train_acc_metric.reset_states()\n",
        "                    print(\"   epoch:%d \\t stp %d trnLoss: %.4f \" % (epoch, stp, float(loss_value)))\n",
        "            else:                    \n",
        "                loss_value = train_step(model,x_batch_train, y_batch_train)                    \n",
        "                train_acc = train_acc_metric.result()\n",
        "                train_acc_metric.reset_states()\n",
        "\n",
        "        # Run a validation loop at the end of each epoch.\n",
        "        for x_batch_val, y_batch_val in val_dataset:\n",
        "            val_loss_value = val_step(model,x_batch_val, y_batch_val)\n",
        "\n",
        "        val_acc = val_acc_metric.result()\n",
        "        val_acc_metric.reset_states()\n",
        "        \n",
        "        # compute time required for each epoch\n",
        "        end_time = time.time() - start_time\n",
        "\n",
        "        print(\"epoch:%d \\t trnLoss: %.4f \\t valLoss: %.4f \\t trnAcc: %.4f \\t valAcc: %.4f \\t time:  %.2f\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "        logFile = open(logFilePath,'a')\n",
        "        logFile.write(\"%d \\t %.4f \\t  %.4f \\t %.4f \\t  %.4f \\t  %.2f \\n\" % (epoch, float(loss_value),float(val_loss_value), float(train_acc), float(val_acc), end_time))\n",
        "        logFile.close()\n",
        "        if epoch % 5 ==0:\n",
        "           # plot the result        \n",
        "           iaPlotLoss(logFilePath)\n",
        "           model.save(modelPath)      \n",
        "    # save the final model\n",
        "    model.save(modelPath)     \n",
        "\n",
        "    # plot the result        \n",
        "    iaPlotLoss(logFilePath)\n",
        "    total_time_end = time.time() - total_time_start\n",
        "    print(\"Training this dataset took \", total_time_end,\" seconds!\") \n",
        "    print(\"Training this dataset took \", total_time_end/60.0,\" minutes!\") \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deIAKC2E5HcT"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwUVTMeg5Ks5"
      },
      "source": [
        "if NNID==2:\n",
        "    # Load the saved model \n",
        "    model = keras.models.load_model(modelPath, compile=False)\n",
        "\n",
        "    start_time = time.time() \n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_tst, y_batch_tst in tst_dataset:\n",
        "        output = model.predict(x_batch_tst)\n",
        "        #y = keras.utils.to_categorical(y_batch_tst)\n",
        "        tst_acc_metric.update_state(y_batch_tst, output)\n",
        "\n",
        "    tst_acc = tst_acc_metric.result()\n",
        "\n",
        "    # compute time required for each epoch\n",
        "    end_time = time.time() - start_time\n",
        "\n",
        "    print(\"test accuracy : %.4f \\t time:  %.2f\" % (  float(tst_acc), end_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-YaiYcJP028"
      },
      "source": [
        "#TODO: show examples using the above datasets\n",
        "# https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRdRNJF2kwU"
      },
      "source": [
        "# More resources:\n",
        "\n",
        "* 3Blue1Brown Neural Network [video tutorials](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) \n",
        "* Deep Learning Video Lectures by Prof. Andreas Maier [Winter 20/21](https://www.youtube.com/watch?v=SCFToE1vM2U&list=PLpOGQvPCDQzvJEPFUQ3mJz72GJ95jyZTh)\n",
        "* Some of the code in this notebook is taken from [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
        "* Calculating number of parameters in [CNN](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d)\n",
        "* Some of the code in this notebook is taken from [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb)\n",
        "* https://nihcc.app.box.com/v/ChestXray-NIHCC\n",
        "* https://www.tensorflow.org/datasets/catalog/patch_camelyon\n",
        "* https://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/\n",
        "* https://elix-tech.github.io/ja/2016/06/02/kaggle-facial-keypoints-ja.html\n",
        "* https://fairyonice.github.io/achieving-top-23-in-kaggles-facial-keypoints-detection-with-keras-tensorflow.html\n",
        "* https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html\n",
        "* https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/object_detection.ipynb\n",
        "* https://github.com/nicknochnack/TFODCourse\n",
        "* https://www.analyticsvidhya.com/blog/2018/10/a-step-by-step-introduction-to-the-basic-object-detection-algorithms-part-1/\n",
        "* https://github.com/enggen/Deep-Learning-Coursera\n",
        "* https://github.com/prateeshreddy/Deep-Learning-Coursera\n",
        "* https://github.com/JudasDie/deeplearning.ai\n"
      ]
    }
  ]
}