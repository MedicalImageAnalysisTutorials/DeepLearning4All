{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IA_DNN_ImageRegistration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMtKRRuWHUCt9Ftf2pI0dlP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MedicalImageAnalysisTutorials/DeepLearning4All/blob/main/IA_DNN_ImageRegistration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLz31jkjk8lh"
      },
      "source": [
        "#  TODO 13.10.2021\n",
        "\n",
        "#  pix2pix check, publish  and run local  (still to do) for Jing  \n",
        "#  classification revisited  (everyone)\n",
        "#  image registration: \n",
        "    # organize and add theory \n",
        "    # spatial transformers \n",
        "\n",
        "\n",
        "#  image registration: \n",
        "#  spatial transformers: https://colab.research.google.com/github/sayakpaul/Spatial-Transformer-Networks-with-Keras/blob/main/STN.ipynb\n",
        "#  voxelmorph: https://github.com/voxelmorph/voxelmorph\n",
        "#  ICNet: https://github.com/zhangjun001/ICNet\n",
        "#  labIRN: https://github.com/cwmok/LapIRN\n",
        "#  LWM https://github.com/idhamari/learning_without_metric?organization=idhamari&organization=idhamari "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "figzmLKBxWC8"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "https://github.com/MedicalImageAnalysisTutorials/ImageRegistrationTutorial/blob/master/m2p_image_registration_example_mse_translation_GradientDescent.ipynb\n",
        "\n",
        "In this notebook, I will try to provide a practical tutorial for deep learning using simple examples. I will try to use simple implementation and avoid using built-in functions to give clear idea about the concept. You need basic programming knowledge. \n",
        "\n",
        "**The Image Registration  Problem:**\n",
        "\n",
        "\n",
        "An example:\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NooQ5tm85ZDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc20173-d74c-4b74-a835-1680eed36ac9"
      },
      "source": [
        "# Setup \n",
        "doInstall =1\n",
        "if doInstall:\n",
        "  !pip install SimpleITK\n",
        "\n",
        "import os, random, time, math, io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 \n",
        "\n",
        "try:   \n",
        "  import SimpleITK as sitk \n",
        "except:\n",
        "  print(\"please install simpleitk\") \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "\n",
        "\n",
        "# to reproduce the same results given same input\n",
        "np.random.seed(1)               \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7WAn3WH3Lpr"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTv78FC4P_J"
      },
      "source": [
        "# Reading and exploring the datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzj-Tvek4V8s"
      },
      "source": [
        "datasetID = 1  # 1:minst is selected by default, for cifar10 use 2\n",
        "NNID      = 1  # 1:NN is by default, for DNN use 2,or 3, for 3D use 4  \n",
        "number_of_classes = 10  # each datasets have 10 classes\n",
        "showSamples = 1\n",
        "# if you have large GPU memory you can combine the images to batches \n",
        "# for faster training.\n",
        "# It is good to try different values\n",
        "batch_size = 2 # you can try larger batch size e.g. 1024 * 6\n",
        "\n",
        "\n",
        "IMG_SIZE = 28  # for minst 28\n",
        "IMG_CH   = 1   # for minst 1\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "doAug = 0\n",
        "\n",
        "# # minst dataset\n",
        "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# class_names = range(10)\n",
        "# if datasetID==2:\n",
        "#     # cifar10 dataset\n",
        "#     # The CIFAR10 dataset contains 60,000 color images in 10 classes, \n",
        "#     # with 6,000 images in each class.\n",
        "#     # The dataset is divided into 50,000 training images and 10,000 testing images.\n",
        "#     # The classes are mutually exclusive and there is no overlap between them.\n",
        "\n",
        "#     (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "#     class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "#                'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "#     if NNID==4:\n",
        "#         #TODO: fix this \n",
        "#         showSamples =0\n",
        "        \n",
        "#         x_train = x_train.reshape(-1,32*32*3) # (32 * 32 * 3)        \n",
        "#         x_train = np.resize(x_train,(x_train.shape[0],15,15,15))        \n",
        "#         x_train = x_train.reshape(-1,15,15,15)\n",
        "#         x_test = x_test.reshape(-1,32*32*3) # (32 * 32 * 3)\n",
        "#         x_test = np.resize(x_test,(x_test.shape[0],15,15,15))\n",
        "#         x_test = x_test.reshape(-1,15,15,15)\n",
        "#         x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "#         y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "#         x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "#         y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "#         print(x_train.shape)\n",
        "#         print(x_test.shape)\n",
        "\n",
        "\n",
        "# # get size \n",
        "# h = x_train.shape[1] # image height\n",
        "# w = x_train.shape[2] # image width\n",
        "# # check for rgb \n",
        "# try:\n",
        "#     # number of channels\n",
        "#     c =  x_train.shape[3]\n",
        "# except:\n",
        "#     # number of channels\n",
        "#     c =  1\n",
        "#     # if there is no number of channels, add 1\n",
        "#     x_train  =  x_train[..., np.newaxis] # np.reshape(x_train, (-1, h,w,1))\n",
        "#     y_train  =  y_train[..., np.newaxis] # np.reshape(y_train, (-1, h,w,1))\n",
        "#     x_test   =  x_test[..., np.newaxis]  # np.reshape(x_test,  (-1, h,w,1))\n",
        "#     y_test   =  y_test[..., np.newaxis]  # np.reshape(y_test,  (-1, h,w,1))\n",
        "\n",
        "\n",
        "# # Reserve 10,000 samples for validation.\n",
        "# x_val = x_train[-10000:]\n",
        "# y_val = y_train[-10000:]\n",
        "# x_train = x_train[:-10000]\n",
        "# y_train = y_train[:-10000]\n",
        "\n",
        "# number_of_pixels = h * w * c\n",
        "\n",
        "\n",
        "# print(\"dataset shape   : \",x_train.shape)\n",
        "# print(\"number of images: \",x_train.shape[0])\n",
        "# print(\"image size      : \",x_train[0].shape)\n",
        "# print(\"image data type : \",type(x_train[0][0][0][0]))\n",
        "# print(\"image max  value: \",np.max(x_train[0]))\n",
        "# print(\"image min  value: \",np.min(x_train[0]))\n",
        "# if c==1:\n",
        "#    print(\"gray or binary image (not color image)\")\n",
        "# elif c==3:\n",
        "#    print(\"rgb color image (or probably non-color image represented with 3 channels)\")\n",
        "\n",
        "\n",
        "# # display sample images \n",
        "# if showSamples:\n",
        "#     plt.figure(figsize=(10,10))\n",
        "#     for i in range(25):\n",
        "#         plt.subplot(5,5,i+1)\n",
        "#         plt.xticks([])\n",
        "#         plt.yticks([])\n",
        "#         plt.grid(False)\n",
        "#         #plt.imshow(x_train[i])\n",
        "#         plt.imshow(cv2.cvtColor(x_train[i], cv2.COLOR_BGR2RGB))\n",
        "\n",
        "#         # The CIFAR labels happen to be arrays, \n",
        "#         # which is why you need the extra index\n",
        "#         if datasetID==1:\n",
        "#             plt.xlabel(y_train[i])\n",
        "#         elif datasetID==2:\n",
        "#             plt.xlabel(class_names[y_train[i][0]])\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# # normalisation\n",
        "# x_train = np.array([ x/255.0 for x in x_train])\n",
        "# x_val   = np.array([ x/255.0 for x in x_val])\n",
        "# x_test  = np.array([ x/255.0 for x in x_test])\n",
        "# #y_train = y_train.astype(np.float32)\n",
        "\n",
        "# # for NN we need 1D \n",
        "# if NNID ==1:\n",
        "#    x_train = np.reshape(x_train, (-1, number_of_pixels))\n",
        "#    x_val   = np.reshape(x_val,  (-1, number_of_pixels))\n",
        "#    x_test  = np.reshape(x_test , (-1, number_of_pixels))\n",
        "\n",
        "# # Prepare the training dataset.\n",
        "# print(x_train.shape,y_train.shape)\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "# train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "# #train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
        "\n",
        "# # Prepare the validation dataset.\n",
        "# val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "# val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# # Prepare the test dataset.\n",
        "# tst_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "# tst_dataset = tst_dataset.batch(batch_size)\n",
        "\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "#(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_train = np.reshape(x_train, (-1, IMG_SIZE, IMG_SIZE, IMG_CH))\n",
        "\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "x_test = np.reshape(x_test, (-1, IMG_SIZE, IMG_SIZE, IMG_CH))\n",
        "\n",
        "# Put aside a few samples to create our validation set\n",
        "val_samples = 2000\n",
        "x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n",
        "new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]\n",
        "\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n",
        "    .shuffle(BATCH_SIZE * 100)\n",
        "    .batch(BATCH_SIZE)\n",
        ")\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP0E_Pzs04y_"
      },
      "source": [
        "# Dataset augmentation\n",
        "\n",
        "It is important to train the model on different variations of the dataset. It is also important to have large datset for training.\n",
        "\n",
        "Using dataset augmentation helps to achieve both of the above goals. From one image, one can generate hundred thousands of images using image transformation.\n",
        "\n",
        "The image transformation could be [spatial transform]() or point transform where we move the points of the image to new locations e.g. shifting, flipping, and/or rotating the imag. \n",
        "\n",
        "Another type of transformation is intensity transform or pixel transform where we change the color values of the pixels in the image e.g. invert the color, add more brightness or darkness. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb2KdoTR07L2"
      },
      "source": [
        "def imagePixelTransforms(img):    \n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    img1   = 1.0- img # invert color\n",
        "    img2   = img +0.3 # more brightness\n",
        "    img3   = img -0.3 # more darkness\n",
        "    images = np.array([img1,img2,img3])\n",
        "    images = [ img.reshape(img.shape) for img in images]\n",
        "\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    return images\n",
        "\n",
        "def imagePointTransforms(img):\n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    # Perform the rotation\n",
        "    center  = (img.shape[0] / 2, img.shape[1] / 2)\n",
        "    sz      = (img.shape[1], img.shape[0])\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 45, 1)\n",
        "    img1 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img1 = img1[...,np.newaxis] if img1.shape !=img.shape else img1\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 90, 1)\n",
        "    img2 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img2 = img2[...,np.newaxis] if img2.shape !=img.shape else img2\n",
        "    tMatrix = cv2.getRotationMatrix2D(center, 270, 1)\n",
        "    img3 = cv2.warpAffine(img, tMatrix, sz)\n",
        "    img3 = img3[...,np.newaxis] if img3.shape !=img.shape else img3\n",
        "\n",
        "    images = np.array([img1,img2,img3])\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    #plt.figure() ;    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # plt.figure() ;    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    return images\n",
        "\n",
        "\n",
        "# define a function for sitk transform\n",
        "def resample(img_array, transform):\n",
        "    # Output image Origin, Spacing, Size, Direction are taken from the reference\n",
        "    # image in this call to Resample\n",
        "    image = sitk.GetImageFromArray(img_array)\n",
        "    reference_image = image\n",
        "    interpolator = sitk.sitkCosineWindowedSinc\n",
        "    default_value = 100.0\n",
        "    resampled_img = sitk.Resample(image, reference_image, transform,\n",
        "                         interpolator, default_value)\n",
        "    resampled_array = sitk.GetArrayFromImage(resampled_img)\n",
        "    return resampled_array\n",
        "\n",
        "def affine_rotate(transform, degrees):\n",
        "    parameters = np.array(transform.GetParameters())\n",
        "    new_transform = sitk.AffineTransform(transform)\n",
        "    dimension =3 \n",
        "    matrix = np.array(transform.GetMatrix()).reshape((dimension,dimension))\n",
        "    radians = -np.pi * degrees / 180.\n",
        "    rotation = np.array([[1  ,0,0], \n",
        "                         [0, np.cos(radians), -np.sin(radians)],\n",
        "                         [0, np.sin(radians), np.cos(radians)]]\n",
        "                        )\n",
        "    new_matrix = np.dot(rotation, matrix)\n",
        "    new_transform.SetMatrix(new_matrix.ravel())\n",
        "    return new_transform\n",
        "\n",
        "\n",
        "def imagePoint3DTransforms(img):\n",
        "    #print(\"imagePoint3DTransforms\")\n",
        "    images = []\n",
        "    # let's make 3 simple transformations\n",
        "    # Perform the rotation\n",
        "    # In SimpleITK resampling convention, the transformation maps points \n",
        "    # from the fixed image to the moving image,\n",
        "    # so inverse of the transform is applied\n",
        "\n",
        "    center = (img.shape[0] /2, img.shape[1] /2,img.shape[1] /2)\n",
        "    rotation_around_center = sitk.AffineTransform(3)\n",
        "    rotation_around_center.SetCenter(center)\n",
        "    \n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -45)\n",
        "    img1 = resample(img, rotation_around_center)\n",
        "\n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -90)\n",
        "    img2 = resample(img, rotation_around_center)\n",
        "\n",
        "    rotation_around_center = affine_rotate(rotation_around_center, -90)\n",
        "    img3 = resample(img, rotation_around_center)\n",
        "\n",
        "    images = np.array([img1,img2,img3])\n",
        "    # plt.figure() ;    plt.imshow(img)\n",
        "    # plt.figure() ;    plt.imshow(img1)\n",
        "    # plt.figure() ;    plt.imshow(img2)\n",
        "    # plt.figure() ;    plt.imshow(img3)    \n",
        "    #plt.figure() ;    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    # plt.figure() ;    plt.imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
        "    return images\n",
        "\n",
        "def doAugmentation(images,labels,batch_size):\n",
        "    # input is an image or a batch e.g. list of images \n",
        "    # get numpy arrays from the tensor    \n",
        "    images = images.numpy()\n",
        "    labels = labels.numpy()\n",
        "    # if 1d convert back to 2d\n",
        "    #print(images.shape)\n",
        "    rgb = 0 ; is3d = 0\n",
        "    if len(images.shape) == 2:\n",
        "       try: \n",
        "          img2d_shape = int(math.sqrt(images.shape[1])) # gray or binary image\n",
        "          images =images.reshape(-1,img2d_shape,img2d_shape)\n",
        "       except:\n",
        "          try: \n",
        "            img2d_shape = int(math.sqrt(images.shape[1]/3)) # rgb image\n",
        "            images =images.reshape(-1,img2d_shape,img2d_shape,3)\n",
        "            rgb = 1  \n",
        "          except:\n",
        "            pass  \n",
        "            # img3d_shape = int(math.sqrt(images.shape[1]/3)) # rgb image\n",
        "            # images =images.reshape(-1,img2d_shape,img2d_shape,3)\n",
        "            # is3d = 1  \n",
        "\n",
        "\n",
        "\n",
        "    x_outputs = [] ; y_outputs = []\n",
        "    i = 0\n",
        "    for img in images:\n",
        "        #print(\"-------------------------\", i ,\"--------------------\")\n",
        "        if NNID==4:\n",
        "           img = img.squeeze() \n",
        "        # from each images we generate 6 images\n",
        "        # 64 batch will generate 448\n",
        "        x_outputs.extend([img])\n",
        "        imgs1 = imagePoint3DTransforms(img)\n",
        "        imgs2 = imagePixelTransforms(img)\n",
        "        #if not rgb:\n",
        "           #imgs1 = np.array( x[...,np.newaxis] for x in imgs1 if len(x.shape)<3) \n",
        "           #imgs2 = np.array( x[...,np.newaxis] for x in imgs2 if len(x.shape)<3)\n",
        "        x_outputs.extend(imgs1) # 3 images\n",
        "        x_outputs.extend(imgs2) # 3 images\n",
        "        # print(img.shape)\n",
        "        # print(imgs1[0].shape)\n",
        "        # print(imgs2[0].shape)\n",
        "        # assign the same label to all transformed images\n",
        "        for j in range ( len(imgs1) +len(imgs2)+1):\n",
        "            y_outputs.extend([labels[i]])\n",
        "\n",
        "        i = i +1\n",
        "    x_outputs = np.array(x_outputs)\n",
        "    if NNID==4:\n",
        "       x_outputs = np.array([x[...,np.newaxis] for x in x_outputs])\n",
        "    y_outputs = np.array(y_outputs)\n",
        "\n",
        "    if (not rgb) and (NNID==1):\n",
        "       x_outputs = np.reshape(x_outputs, (-1,img2d_shape*img2d_shape,1))\n",
        "    elif (rgb) and (NNID==1):\n",
        "       x_outputs = np.reshape(x_outputs, (-1,img2d_shape*img2d_shape*3))   \n",
        "\n",
        "    new_train_dataset = tf.data.Dataset.from_tensor_slices((x_outputs, y_outputs))\n",
        "    new_train_dataset = new_train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "    return new_train_dataset"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZY4NXWJ4Xoq"
      },
      "source": [
        "# Spatial Transform Network (STN)\n",
        "\n",
        "https://colab.research.google.com/github/sayakpaul/Spatial-Transformer-Networks-with-Keras/blob/main/STN.ipynb\n",
        "\n",
        "\n",
        "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/spatial_transformer_tutorial.ipynb\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdksy7n-vhhe"
      },
      "source": [
        "## HyperParameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIES-N61vkTV"
      },
      "source": [
        "AUTO = tf.data.AUTOTUNE\n",
        "EPOCHS = 30"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ014XCWBnIe"
      },
      "source": [
        "## Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vmLdxrhBpUV"
      },
      "source": [
        "# model creation\n",
        "\n",
        "\n",
        "\n",
        "#----------------------------------------------------\n",
        "# Grid Generator\n",
        "# generate a deformation field using transformation matrix theta\n",
        "#----------------------------------------------------\n",
        "def affine_grid_generator(height, width, theta):\n",
        "    \"\"\"\n",
        "    This function returns a sampling grid, which when\n",
        "    used with the bilinear sampler on the input feature\n",
        "    map, will create an output feature map that is an\n",
        "    affine transformation [1] of the input feature map.\n",
        "    Input\n",
        "    -----\n",
        "    - height: desired height of grid/output. Used\n",
        "      to downsample or upsample.\n",
        "    - width: desired width of grid/output. Used\n",
        "      to downsample or upsample.\n",
        "    - theta: affine transform matrices of shape (num_batch, 2, 3).\n",
        "      For each image in the batch, we have 6 theta parameters of\n",
        "      the form (2x3) that define the affine transformation T.\n",
        "    Returns\n",
        "    -------\n",
        "    - normalized grid (-1, 1) of shape (num_batch, 2, H, W).\n",
        "      The 2nd dimension has 2 components: (x, y) which are the\n",
        "      sampling points of the original image for each point in the\n",
        "      target image.\n",
        "    Note\n",
        "    ----\n",
        "    [1]: the affine transformation allows cropping, translation,\n",
        "         and isotropic scaling.\n",
        "    \"\"\"\n",
        "    num_batch = tf.shape(theta)[0]\n",
        "\n",
        "    # create normalized 2D grid\n",
        "    x = tf.linspace(-1.0, 1.0, width)\n",
        "    y = tf.linspace(-1.0, 1.0, height)\n",
        "    x_t, y_t = tf.meshgrid(x, y)\n",
        "\n",
        "    # flatten\n",
        "    x_t_flat = tf.reshape(x_t, [-1])\n",
        "    y_t_flat = tf.reshape(y_t, [-1])\n",
        "\n",
        "    # reshape to [x_t, y_t , 1] - (homogeneous form)\n",
        "    ones = tf.ones_like(x_t_flat)\n",
        "    sampling_grid = tf.stack([x_t_flat, y_t_flat, ones])\n",
        "\n",
        "    # repeat grid num_batch times\n",
        "    sampling_grid = tf.expand_dims(sampling_grid, axis=0)\n",
        "    sampling_grid = tf.tile(sampling_grid, tf.stack([num_batch, 1, 1]))\n",
        "\n",
        "    # cast to float32 (required for matmul)\n",
        "    theta = tf.cast(theta, 'float32')\n",
        "    sampling_grid = tf.cast(sampling_grid, 'float32')\n",
        "\n",
        "    # transform the sampling grid - batch multiply\n",
        "    batch_grids = tf.matmul(theta, sampling_grid)\n",
        "    # batch grid has shape (num_batch, 2, H*W)\n",
        "\n",
        "    # reshape to (num_batch, H, W, 2)\n",
        "    batch_grids = tf.reshape(batch_grids, [num_batch, 2, height, width])\n",
        "\n",
        "    return batch_grids    \n",
        "\n",
        "#----------------------------------------------------\n",
        "# BiLinear Sampler\n",
        "# transform an image using deformation field \n",
        "#----------------------------------------------------\n",
        "def bilinear_sampler(img, defField):\n",
        "    \"\"\"\n",
        "    Performs bilinear sampling of the input images according to the\n",
        "    normalized coordinates provided by the sampling grid. Note that\n",
        "    the sampling is done identically for each channel of the input.\n",
        "    To test if the function works properly, output image should be\n",
        "    identical to input image when theta is initialized to identity\n",
        "    transform.\n",
        "    Input\n",
        "    -----\n",
        "    - img: batch of images in (B, H, W, C) layout.\n",
        "    - grid: x, y which is the output of affine_grid_generator.\n",
        "    Returns\n",
        "    -------\n",
        "    - out: interpolated images according to grids. Same size as grid.\n",
        "    \"\"\"\n",
        "\n",
        "    def get_pixel_value(img, x, y):\n",
        "        \"\"\"\n",
        "        Utility function to get pixel value for coordinate\n",
        "        vectors x and y from a  4D tensor image.\n",
        "        Input\n",
        "        -----\n",
        "        - img: tensor of shape (B, H, W, C)\n",
        "        - x: flattened tensor of shape (B*H*W,)\n",
        "        - y: flattened tensor of shape (B*H*W,)\n",
        "        Returns\n",
        "        -------\n",
        "        - output: tensor of shape (B, H, W, C)\n",
        "        \"\"\"\n",
        "        shape = tf.shape(x)\n",
        "        batch_size = shape[0]\n",
        "        height = shape[1]\n",
        "        width = shape[2]\n",
        "\n",
        "        batch_idx = tf.range(0, batch_size)\n",
        "        batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
        "        b = tf.tile(batch_idx, (1, height, width))\n",
        "\n",
        "        indices = tf.stack([b, y, x], 3)\n",
        "\n",
        "        return tf.gather_nd(img, indices)\n",
        "\n",
        "    x =  defField[:, 0, :, :]\n",
        "    y =  defField[:, 1, :, :]\n",
        "    H = tf.shape(img)[1]\n",
        "    W = tf.shape(img)[2]\n",
        "    max_y = tf.cast(H - 1, 'int32')\n",
        "    max_x = tf.cast(W - 1, 'int32')\n",
        "    zero = tf.zeros([], dtype='int32')\n",
        "\n",
        "    # rescale x and y to [0, W-1/H-1]\n",
        "    x = tf.cast(x, 'float32')\n",
        "    y = tf.cast(y, 'float32')\n",
        "    x = 0.5 * ((x + 1.0) * tf.cast(max_x-1, 'float32'))\n",
        "    y = 0.5 * ((y + 1.0) * tf.cast(max_y-1, 'float32'))\n",
        "\n",
        "    # grab 4 nearest corner points for each (x_i, y_i)\n",
        "    x0 = tf.cast(tf.floor(x), 'int32')\n",
        "    x1 = x0 + 1\n",
        "    y0 = tf.cast(tf.floor(y), 'int32')\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    # clip to range [0, H-1/W-1] to not violate img boundaries\n",
        "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
        "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
        "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
        "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
        "\n",
        "    # get pixel value at corner coords\n",
        "    Ia = get_pixel_value(img, x0, y0)\n",
        "    Ib = get_pixel_value(img, x0, y1)\n",
        "    Ic = get_pixel_value(img, x1, y0)\n",
        "    Id = get_pixel_value(img, x1, y1)\n",
        "\n",
        "    # recast as float for delta calculation\n",
        "    x0 = tf.cast(x0, 'float32')\n",
        "    x1 = tf.cast(x1, 'float32')\n",
        "    y0 = tf.cast(y0, 'float32')\n",
        "    y1 = tf.cast(y1, 'float32')\n",
        "\n",
        "    # calculate deltas\n",
        "    wa = (x1-x) * (y1-y)\n",
        "    wb = (x1-x) * (y-y0)\n",
        "    wc = (x-x0) * (y1-y)\n",
        "    wd = (x-x0) * (y-y0)\n",
        "\n",
        "    # add dimension for addition\n",
        "    wa = tf.expand_dims(wa, axis=3)\n",
        "    wb = tf.expand_dims(wb, axis=3)\n",
        "    wc = tf.expand_dims(wc, axis=3)\n",
        "    wd = tf.expand_dims(wd, axis=3)\n",
        "\n",
        "    # compute output\n",
        "    out = tf.add_n([wa*Ia, wb*Ib, wc*Ic, wd*Id])\n",
        "\n",
        "    return out   \n",
        "\n",
        "#----------------------------------------------------\n",
        "# Spatial transformer network forward function\n",
        "#----------------------------------------------------\n",
        "def stn(inputs):\n",
        "    # we can test using stn or not \n",
        "    useSTN = 1\n",
        "    if useSTN:\n",
        "        #----------------------------------------------------\n",
        "        # Spatial transformer localization-network\n",
        "        #  2 conv layers followed by maxpooling\n",
        "        #  the output is the transformation parameters?\n",
        "        #----------------------------------------------------\n",
        "        x10 =  tf.keras.Sequential([\n",
        "            layers.Conv2D(8, kernel_size=7, input_shape=(IMG_SIZE, IMG_SIZE, IMG_CH), \n",
        "                        activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "            layers.MaxPool2D(strides=2),\n",
        "\n",
        "            layers.Conv2D(10, kernel_size=5, \n",
        "                        activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "            layers.MaxPool2D(strides=2),\n",
        "        ])(inputs)\n",
        "\n",
        "        #----------------------------------------------------\n",
        "        # Regressor for the 3 * 2 affine matrix\n",
        "        #  we initialize with identity matrix to get a similar image\n",
        "        #----------------------------------------------------       \n",
        "        x20 = tf.reshape(x10, (-1, 10 * 3 * 3 ))\n",
        "        \n",
        "        tMatrix =  tf.keras.Sequential([\n",
        "            layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "            layers.Dense(3 * 2, kernel_initializer=\"zeros\", \n",
        "                         bias_initializer=tf.keras.initializers.Constant([1, 0, 0, 0, 1, 0]))\n",
        "        ])(x20)\n",
        "        # reshape to output an affine transformation matrix \n",
        "        tMatrix = tf.reshape(tMatrix, (-1, 2, 3))\n",
        "        # we can output the transfrom parameters here \n",
        "\n",
        "        # deformation field? using image size and the transformation matrix\n",
        "        grid = affine_grid_generator(IMG_SIZE, IMG_SIZE, tMatrix)\n",
        "        # x_s = grid[:, 0, :, :]\n",
        "        # y_s = grid[:, 1, :, :]\n",
        "        # transform the image using the deformation field        \n",
        "        new_image = bilinear_sampler(inputs, grid)\n",
        "        # we can output the transformed image here \n",
        "    else:\n",
        "        new_image = inputs\n",
        "\n",
        "    # this is just a classification network\n",
        "    x41 = layers.Conv2D(10, (5, 5), activation=\"relu\", kernel_initializer=\"he_normal\")(new_image)\n",
        "    x42 = layers.MaxPooling2D(pool_size=(2, 2))(x41)\n",
        "    \n",
        "    x51 = layers.Conv2D(20, (5, 5), activation=\"relu\", kernel_initializer=\"he_normal\")(x42)\n",
        "    x52 = layers.SpatialDropout2D(0.5)(x51)\n",
        "    x53 = layers.MaxPooling2D(pool_size=(2, 2))(x52)\n",
        "    \n",
        "    x61 = tf.reshape(x53, (-1, 320))\n",
        "    x62 = layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\")(x61)\n",
        "    x63 = layers.Dropout(0.5)(x62)    \n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x63)\n",
        "    model = tf.keras.Model(inputs, outputs)    \n",
        "    return model\n",
        "\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pxQZGiPwGuN"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "hgBkUhMnwI_J",
        "outputId": "f29d45ca-5b5d-4e66-d3ee-115bf09ffa14"
      },
      "source": [
        "# num_params_w_stn = get_classification_model().count_params()\n",
        "# print(f\"Number of parameters    with STN: {num_params_w_stn / 1e6} M\")\n",
        "\n",
        "# transformed_x = stn(np.expand_dims(x_train[0], 0))\n",
        "# plt.imshow(transformed_x.numpy().squeeze(), cmap=\"binary\")\n",
        "# plt.show()\n",
        "# plt.imshow(x_train[0].squeeze(), cmap=\"binary\")\n",
        "# plt.show()\n",
        "\n",
        "\n",
        "\n",
        "inputs = layers.Input((IMG_SIZE, IMG_SIZE, IMG_CH))    \n",
        "stn_model = stn(inputs)\n",
        "\n",
        "# stn_mini_model = tf.keras.Model(stn_model.input, stn_model.layers[-10].output)\n",
        "# file_writer_cm = tf.summary.create_file_writer(\"logs/stn_progress\")\n",
        "\n",
        "# # Courtesy: https://www.tensorflow.org/tensorboard/image_summaries\n",
        "# def plot_to_image(figure):\n",
        "#     # Save the plot to a PNG in memory.\n",
        "#     buf = io.BytesIO()\n",
        "#     plt.savefig(buf, format='png')\n",
        "    \n",
        "#     # Closing the figure prevents it from being displayed directly inside\n",
        "#     # the notebook.\n",
        "#     plt.close(figure)\n",
        "#     buf.seek(0)\n",
        "    \n",
        "#     # Convert PNG buffer to TF image\n",
        "#     image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
        "    \n",
        "#     # Add the batch dimension\n",
        "#     image = tf.expand_dims(image, 0)\n",
        "#     return image\n",
        "\n",
        "# def log_progression(epoch, logs):\n",
        "#     sampled_transformed = stn_mini_model(x_test[:25])\n",
        "    \n",
        "#     figure = plt.figure(figsize=(10, 10))\n",
        "#     for n in range(25):\n",
        "#         plt.subplot(5, 5, n+1)\n",
        "#         plt.imshow(sampled_transformed[n].numpy().squeeze(), cmap=\"binary\")\n",
        "#         plt.axis(\"off\")\n",
        "\n",
        "#     progress_image = plot_to_image(figure)\n",
        "#     with file_writer_cm.as_default():\n",
        "#         tf.summary.image(\"Progression\", progress_image, step=epoch)\n",
        "\n",
        "# # Define the per-epoch callback.\n",
        "# progress_callback = tf.keras.callbacks.LambdaCallback(on_epoch_end=log_progression)\n",
        "\n",
        "# es_callback = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "# reduce_lr = tf.keras.callbacks.ReduceLROnPlateau()\n",
        "\n",
        "stn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# stn_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[es_callback, reduce_lr, progress_callback])%\n",
        "stn_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS)\n",
        "_, test_acc = stn_model.evaluate(test_ds)\n",
        "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "454/454 [==============================] - ETA: 0s - loss: 0.8085 - accuracy: 0.7349"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-68ba3b2ab0ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mstn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# stn_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=[es_callback, reduce_lr, progress_callback])%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mstn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy: {:.2f}%\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1224\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m               \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m               _use_cached_eval_dataset=True)\n\u001b[0m\u001b[1;32m   1227\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpxH1ntsqfuZ"
      },
      "source": [
        "# TODO\n",
        "# NN pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmAbPxuDqrVV"
      },
      "source": [
        "# TODO\n",
        "# Pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mox2km11qyqY"
      },
      "source": [
        "##TODO pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlx1hVMQGAbR"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpZmXOUfrDho"
      },
      "source": [
        "# TODO Pytorch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8X6Da244p-r"
      },
      "source": [
        "# VoxelMorph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2a1zuKu4_ZR"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD_BI-0C4_kr"
      },
      "source": [
        "# ICNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKpCUhIT5EWp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwUVTMeg5Ks5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dne9y6mbMW0V"
      },
      "source": [
        "#  \n",
        "Training large datsets takes hours. It is good to check the [training curves](https://en.wikipedia.org/wiki/Learning_curve) and stop the training if it is a waste of time and resources. To do this one should check the training curves and identify specific problems.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-YaiYcJP028"
      },
      "source": [
        "#TODO: show examples using the above datasets\n",
        "# https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j-OqIpdOlJf"
      },
      "source": [
        "# LabIRN\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ojw1-BrOrzb"
      },
      "source": [
        "# LWM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeRdRNJF2kwU"
      },
      "source": [
        "# More resources:\n",
        "\n",
        "* 3Blue1Brown Neural Network [video tutorials](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi) \n",
        "* Deep Learning Video Lectures by Prof. Andreas Maier [Winter 20/21](https://www.youtube.com/watch?v=SCFToE1vM2U&list=PLpOGQvPCDQzvJEPFUQ3mJz72GJ95jyZTh)\n",
        "* Some of the code in this notebook is taken from [here](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)\n",
        "* Calculating number of parameters in [CNN](https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d)\n",
        "* Some of the code in this notebook is taken from [here](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb)\n",
        "* https://imerit.net/blog/top-13-machine-learning-image-classification-datasets-all-pbm/\n",
        "* https://nihcc.app.box.com/v/ChestXray-NIHCC\n",
        "* https://www.kaggle.com/xhlulu/recursion-cellular-image-classification-224-jpg\n",
        "* https://www.tensorflow.org/datasets/catalog/patch_camelyon\n"
      ]
    }
  ]
}